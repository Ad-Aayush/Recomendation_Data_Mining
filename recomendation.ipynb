{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer = pd.read_csv('csv/Posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = question_answer[question_answer['PostTypeId'] == 2]\n",
    "questions = question_answer[question_answer['PostTypeId'] == 1]\n",
    "# this observation has come from the fact that, PostTypeId == 2, have a parent Id, while those with 1 have answerCount field non empty, rest all ids are wikis etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerer = answers.groupby('OwnerUserId')['ParentId'].apply(list).to_dict()\n",
    "answerer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tags = questions['Tags'].apply(lambda x: list(filter(lambda x: x != '', x.split('|'))))\n",
    "question_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_tags = question_tags.explode()\n",
    "exploded_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = exploded_tags.value_counts()\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tag = tags.to_dict()\n",
    "count_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.isna().sum()\n",
    "# 6822 answers are such that their owner is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_answers_per_user = answers['OwnerUserId'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_answers_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wrong code, gives very wrong answers, design is the most used tag\n",
    "\n",
    "# count_tag = dict()\n",
    "# for tags in question_tags:\n",
    "#   for tag in tags:\n",
    "#     if tag not in count_tag:\n",
    "#       count_tag[tag] = 0\n",
    "#     count_tag[tag] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.read_csv('csv/Tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags_df.set_index('Id', inplace=True)\n",
    "tag_cols = tags_df.columns\n",
    "tag_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_tag_cols = ['Id', 'TagName']\n",
    "drop_cols = list(filter(lambda x: x not in req_tag_cols, tag_cols))\n",
    "drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tags_df = tags_df.drop(drop_cols, axis=1)\n",
    "new_tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tags_df.set_index('TagName', inplace=True)\n",
    "new_tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = new_tags_df.to_dict()['Id']\n",
    "tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tag = {tag_dict[k]: v for k, v in count_tag.items() if k in tag_dict} # some tags not there in the tags.csv file, don't know why :(\n",
    "count_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerer_table = pd.DataFrame(count_answers_per_user.items(), columns=['UserId', 'AnsweredQuestionCount'])\n",
    "answerer_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_table = pd.DataFrame(count_tag.items(), columns=['TagId', 'Count'])\n",
    "tags_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tags_table = tags_table.sort_values(by='Count', ascending=False)\n",
    "sorted_tags_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_answerer_table = answerer_table.sort_values(by='AnsweredQuestionCount', ascending=False)\n",
    "sorted_answerer_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerer_table = answerer_table[answerer_table['AnsweredQuestionCount'] >= threshold]\n",
    "answerer_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_table = tags_table[tags_table['Count'] >= threshold]\n",
    "tags_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tag_count = {}\n",
    "tags_table = tags_table.sort_values(['TagId'])\n",
    "answerer_table = answerer_table.sort_values(['UserId'])\n",
    "users = answerer_table['UserId'].values\n",
    "tags = tags_table['TagId'].values\n",
    "q_tag_map = question_tags.to_dict()\n",
    "\n",
    "\n",
    "for user in users:\n",
    "  tag_count = {tag : 0 for tag in tags}\n",
    "  for question in answerer[user]:\n",
    "    for tag in q_tag_map[question]:\n",
    "      try:\n",
    "        tag_count[tag_dict[tag]] += 1\n",
    "      except KeyError:\n",
    "        pass\n",
    "  user_tag_count[user] = tag_count\n",
    "del q_tag_map\n",
    "del users\n",
    "del tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = pd.DataFrame.from_dict(user_tag_count, orient='index')\n",
    "del user_tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert.shape # 1163 users, 973 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = expert.map(lambda x: x/3 if x < 15 else 5)\n",
    "expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_matrix = expert.to_numpy()\n",
    "expert_shape = expert_matrix.shape\n",
    "expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_matrix[expert_matrix == 0] = np.nan\n",
    "np.isnan(expert_matrix).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = (int(0.85 *expert_shape[0] ), int(0.85*expert_shape[1]))\n",
    "test_users, test_tags = test_start\n",
    "test_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = np.copy(expert_matrix[test_users:, test_tags:])\n",
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = np.copy(expert_matrix)\n",
    "train_matrix[test_users:, test_tags:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Report the following about your utility matrix:\n",
    "Summation value of the utility matrix\n",
    "Highest row sum of the utility matrix\n",
    "Highest column sum of the utility matrix\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sum_utility = np.nansum(expert_matrix)\n",
    "highest_row_sum = np.nansum(expert_matrix, axis = 0).max()\n",
    "highest_col_sum = np.nansum(expert_matrix, axis = 1).max()\n",
    "sum_utility, highest_row_sum, highest_col_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Report the following for your train and test data\n",
    "Summation value of the train matrix\n",
    "Dimension of the test matrix\n",
    "Summation value of test matrix\n",
    "\"\"\"\n",
    "\n",
    "sum_train = np.nansum(train_matrix)\n",
    "dim_test = test_matrix.shape\n",
    "sum_test = test_matrix.sum()\n",
    "sum_train, dim_test, sum_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users_start, test_tags_start = test_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "class CollaborativeFilter():\n",
    "  def __init__(self, utility_matrix, function='weighted'):\n",
    "    utility_zero_mask = utility_matrix == 0\n",
    "    utility_matrix[utility_zero_mask] = np.nan\n",
    "    self.utility_matrix = utility_matrix\n",
    "    self.utility_matrix_filled = utility_matrix.copy()\n",
    "    self.epsilon = 1e-9 # a small value to avoid division by zero\n",
    "    \n",
    "    self.final_rating_function = None\n",
    "    if function == 'weighted':\n",
    "      self.final_rating_function = self.weighted_average\n",
    "    elif function=='regular_average':\n",
    "      self.final_rating_function = self.average\n",
    "    else:\n",
    "      raise Exception(\"Function not allowed\")\n",
    "    \n",
    "  \n",
    "  def weighted_average(self, vector: np.ndarray, scores: np.ndarray):\n",
    "    return np.dot(vector, scores)/(np.sum(scores) + self.epsilon)\n",
    "  \n",
    "  def average(self, vector: np.ndarray, scores: np.ndarray):\n",
    "    return np.mean(vector)\n",
    "    \n",
    "\n",
    "  @abstractmethod\n",
    "  def predict(self):\n",
    "    pass\n",
    "  \n",
    "  \n",
    "  @abstractmethod\n",
    "  def compute_similarities(self):\n",
    "    pass\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedCollaborativeFilter(CollaborativeFilter):\n",
    "    def __init__(self,expert_matrix ,k , function):\n",
    "      \"\"\"\n",
    "      utility_matrix : 2D numpy array\n",
    "      k : int\n",
    "      \n",
    "      k is the number of similar users to consider for prediction\n",
    "      utility_matrix is the matrix of user ratings, nan filled with 0\n",
    "      \"\"\"\n",
    "      super().__init__(expert_matrix, function)\n",
    "      self.k = k\n",
    "    \n",
    "    def compute_similarities(self, user_vector : np.ndarray):\n",
    "      \"\"\"\n",
    "      user_vector : 1D numpy array\n",
    "      \"\"\"\n",
    "      # self.utility_matrix has each column as a user and each row as an item\n",
    "      utility_rating_means = np.nanmean(self.utility_matrix, axis=0)\n",
    "      user_rating_mean = np.nanmean(user_vector)\n",
    "      # subtract the mean rating of each user from their ratings\n",
    "      utility_matrix_centered = self.utility_matrix - utility_rating_means\n",
    "      \n",
    "      # compute the cosine similarity between user_vector and each user\n",
    "      user_vector_centered = user_vector - user_rating_mean\n",
    "      \n",
    "      # compute the norms of the vectors, which might have nans as well\n",
    "      user_vector_norm = np.sqrt(np.nansum(user_vector_centered**2))\n",
    "      utility_matrix_norm = np.sqrt(np.nansum(utility_matrix_centered**2, axis=1))\n",
    "      # compute the dot product between user_vector and each user\n",
    "      \n",
    "      dot_product = np.nansum(utility_matrix_centered *user_vector_centered, axis = 1)\n",
    "      # compute the cosine similarity\n",
    "      similarities = dot_product/(user_vector_norm * utility_matrix_norm + self.epsilon)\n",
    "      \n",
    "      return similarities\n",
    "    \n",
    "    def find_k_nearest_users(self, user_vector : np.ndarray, i: int):\n",
    "      \"\"\"\n",
    "      user_vector : 1D numpy array\n",
    "      \n",
    "      returns the indices of the k most similar users to user_vector\n",
    "      \"\"\"\n",
    "      \n",
    "      similarities = self.compute_similarities(user_vector)\n",
    "      users_to_consider = np.where(~np.isnan(self.utility_matrix[:, i]))[0]\n",
    "      k = min(self.k, len(users_to_consider))\n",
    "      sorted_indices = np.argsort(similarities[users_to_consider])[-k:]\n",
    "      users_to_consider = users_to_consider[sorted_indices]\n",
    "      similarities = similarities[users_to_consider]\n",
    "      return similarities, users_to_consider\n",
    "    \n",
    "    \n",
    "    def predict(self, user_vector : np.ndarray, i: int):\n",
    "      \"\"\"\n",
    "      user_vector : 1D numpy array\n",
    "      i : int\n",
    "      \n",
    "      i is the index of the item to predict the rating for\n",
    "      \"\"\"\n",
    "      \n",
    "      # find the k most similar users who rated this item \n",
    "      similarities, users_to_consider = self.find_k_nearest_users(user_vector, i)\n",
    "      # similarity_scores = similarities[users_to_consider]\n",
    "      \n",
    "      # predict the rating for the item\n",
    "      prediction = self.final_rating_function(self.utility_matrix[users_to_consider, i], similarities)\n",
    "      return prediction\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_user_collaborator(train_matrix: np.ndarray, expert_matrix: np.ndarray, user_start: int, item_start: int, model: UserBasedCollaborativeFilter) -> float:\n",
    "    rmse_loss = 0\n",
    "    count = 0\n",
    "    for i in range(user_start, expert_matrix.shape[0]):\n",
    "        for j in range(item_start, expert_matrix.shape[1]):\n",
    "            if not np.isnan(expert_matrix[i, j]):\n",
    "                prediction = model.predict(train_matrix[i], j)\n",
    "                # print(prediction, i, j)\n",
    "                rmse_loss += (prediction - expert_matrix[i, j])**2\n",
    "                count += 1\n",
    "    rmse_loss = np.sqrt(rmse_loss/count)\n",
    "    \n",
    "    return rmse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cf = UserBasedCollaborativeFilter(train_matrix, 5, 'regular_average')\n",
    "test_user_collaborator(train_matrix, expert_matrix, test_users_start, test_tags_start, user_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cf = UserBasedCollaborativeFilter(train_matrix, 5, 'weighted')\n",
    "test_user_collaborator(train_matrix, expert_matrix, test_users_start, test_tags_start, user_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ItemBasedCollaborativeFilter(CollaborativeFilter):\n",
    "    def __init__(self,expert_matrix ,k, function):\n",
    "      \"\"\"\n",
    "      utility_matrix : 2D numpy array\n",
    "      k : int\n",
    "      \n",
    "      k is the number of similar items to consider for prediction\n",
    "      utility_matrix is the matrix of user ratings, nan filled with 0\n",
    "      \"\"\"\n",
    "      super().__init__(expert_matrix, function)\n",
    "      self.k = k\n",
    "    \n",
    "    def compute_similarities(self, item_vector : np.ndarray):\n",
    "      \"\"\"\n",
    "      item_vector : 1D numpy array\n",
    "      \n",
    "      \"\"\"\n",
    "      # self.utility_matrix has each column as a user and each row as an item\n",
    "      utility_rating_means = np.nanmean(self.utility_matrix.T, axis=1)\n",
    "      item_rating_mean = np.nanmean(item_vector)\n",
    "      # subtract the mean rating of each user from their ratings\n",
    "      utility_matrix_centered = self.utility_matrix.T - utility_rating_means[:, np.newaxis]\n",
    "      \n",
    "      # compute the cosine similarity between item_vector and each item\n",
    "      item_vector_centered = item_vector - item_rating_mean\n",
    "      \n",
    "      # compute the norms of the vectors, which might have nans as well\n",
    "      item_vector_norm = np.sqrt(np.nansum(item_vector_centered**2))\n",
    "      utility_matrix_norm = np.sqrt(np.nansum(utility_matrix_centered**2, axis=1))\n",
    "      \n",
    "      # compute the dot product between item_vector and each item\n",
    "      dot_product = np.nansum(utility_matrix_centered *item_vector_centered, axis=1)\n",
    "      \n",
    "      # compute the cosine similarity\n",
    "      similarities = dot_product/(item_vector_norm * utility_matrix_norm + self.epsilon)\n",
    "      \n",
    "      return similarities\n",
    "    \n",
    "    \n",
    "    def find_k_nearest_items(self, item_vector : np.ndarray, u: int):\n",
    "      \"\"\"\n",
    "      item_vector : 1D numpy array\n",
    "      \n",
    "      returns the indices of the k most similar items to item_vector\n",
    "      \"\"\"\n",
    "      \n",
    "      similarities = self.compute_similarities(item_vector)\n",
    "      items_to_consider = np.where(~np.isnan(self.utility_matrix[u]))[0]\n",
    "      k = min(self.k, len(items_to_consider))\n",
    "      sorted_indices = np.argsort(similarities[items_to_consider])[-k:]\n",
    "      items_to_consider = items_to_consider[sorted_indices]\n",
    "      similarities = similarities[items_to_consider]\n",
    "      return similarities, items_to_consider\n",
    "    \n",
    "    \n",
    "    def predict(self, item_vector : np.ndarray, u: int):\n",
    "      \"\"\"\n",
    "      item_vector : 1D numpy array\n",
    "      u : int\n",
    "      u is the index of the user to predict the rating for\n",
    "      \"\"\"\n",
    "      similarity_scores, items_to_consider = self.find_k_nearest_items(item_vector, u)\n",
    "      \n",
    "      # predict the rating for the item\n",
    "      prediction = self.final_rating_function(self.utility_matrix[u, items_to_consider], similarity_scores)\n",
    "      return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_item_item_filter(train_matrix: np.ndarray, expert_matrix: np.ndarray, user_start: int, item_start: int, model: ItemBasedCollaborativeFilter) -> float:\n",
    "    rmse_loss = 0\n",
    "    count = 0\n",
    "    for i in range(user_start, expert_matrix.shape[0]):\n",
    "        for j in range(item_start, expert_matrix.shape[1]):\n",
    "            if not np.isnan(expert_matrix[i, j]):\n",
    "                prediction = model.predict(train_matrix[:, j], i)\n",
    "                # print(prediction, i, j)\n",
    "                rmse_loss += (prediction - expert_matrix[i, j])**2\n",
    "                count += 1\n",
    "    rmse_loss = np.sqrt(rmse_loss/count)\n",
    "    \n",
    "    return rmse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cf = ItemBasedCollaborativeFilter(train_matrix, 5, 'weighted')\n",
    "test_item_item_filter(train_matrix, expert_matrix, test_users_start, test_tags_start, item_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cf = ItemBasedCollaborativeFilter(train_matrix, 5, 'regular_average')\n",
    "test_item_item_filter(train_matrix, expert_matrix, test_users_start, test_tags_start, item_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorDecomposition:\n",
    "    def __init__(self, utility_matrix: np.ndarray, f : int, regp :int= 0, regq:int = 0):\n",
    "        \"\"\"\n",
    "        utility_matrix : 2D numpy array, Expert matrix\n",
    "        f : Number of Latent Factors\n",
    "        regp: regularization rate over p\n",
    "        regq: regularization rate over q\n",
    "        \"\"\"\n",
    "        self.utility_matrix = utility_matrix\n",
    "        self.f = f\n",
    "        self.epsilon = 1e-9\n",
    "        self.regp = regp\n",
    "        self.regq = regq\n",
    "        num_users, num_items = utility_matrix.shape\n",
    "        self.P = np.random.randn(num_users, f)/np.sqrt(num_users*f)\n",
    "        self.Q = np.random.randn(num_items, f)/np.sqrt(num_items*f)\n",
    "    \n",
    "    def predict(self, u, i):\n",
    "        return np.dot(self.P[u], self.Q[i])\n",
    "    \n",
    "    def train(self, epochs = 1000):\n",
    "        \n",
    "        learn_start = 0.0001 # please keep these low while testing higher values tend to overshoot\n",
    "        learn_end = 0.0000001\n",
    "        for epoch in range(epochs):\n",
    "            alpha = learn_start - (learn_start - learn_end)*epoch/epochs\n",
    "            pred_matrix = np.dot(self.P, self.Q.T)\n",
    "            diffs = pred_matrix - self.utility_matrix\n",
    "            loss = np.nansum(diffs**2) + self.regp*np.nansum(self.P**2) + self.regq*np.nansum(self.Q**2)\n",
    "            \n",
    "            # applying SGD\n",
    "            p_reg_changes = 2*self.regp*self.P \n",
    "            q_reg_changes = 2*self.regq*self.Q\n",
    "            \n",
    "            \n",
    "            diffs[np.isnan(diffs)] = 0\n",
    "            # from observation each p[i,j] affects prediction of only user i for each item k. So, its SGD will get changes for user i and each item k.\n",
    "            p_rmse_changes = 2*(np.dot(diffs, self.Q))\n",
    "            # from observation each q[i,j] affects prediction of only item i for each user k. So, its SGD will get changes for item i and each user k.\n",
    "            q_rmse_changes = 2*(np.dot(diffs.T, self.P))\n",
    "            \n",
    "            self.P -= alpha*(p_rmse_changes + p_reg_changes)\n",
    "            self.Q -= alpha*(q_rmse_changes + q_reg_changes)\n",
    "                            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} : Loss = {loss}\")\n",
    "            \n",
    "    def get_factors(self):\n",
    "        return self.P, self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lfd(model: LatentFactorDecomposition, expert_matrix: np.ndarray, user_start: int, item_start : int,f = 5):\n",
    "    rmse_loss = 0\n",
    "    count = 0\n",
    "    for i in range(user_start, expert_matrix.shape[0]):\n",
    "        for j in range(item_start, expert_matrix.shape[1]):\n",
    "            if not np.isnan(expert_matrix[i, j]):\n",
    "                prediction = model.predict(i, j)\n",
    "                # print(prediction, i, j)\n",
    "                rmse_loss += (prediction - expert_matrix[i, j])**2\n",
    "                count += 1\n",
    "    rmse_loss = np.sqrt(rmse_loss/count)\n",
    "    return rmse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 : Loss = 143751.5457093082\n",
      "Epoch 2/1000 : Loss = 143751.4119850054\n",
      "Epoch 3/1000 : Loss = 143751.275243083\n",
      "Epoch 4/1000 : Loss = 143751.1344177798\n",
      "Epoch 5/1000 : Loss = 143750.98840643294\n",
      "Epoch 6/1000 : Loss = 143750.8360564811\n",
      "Epoch 7/1000 : Loss = 143750.67615205163\n",
      "Epoch 8/1000 : Loss = 143750.50739997148\n",
      "Epoch 9/1000 : Loss = 143750.32841504045\n",
      "Epoch 10/1000 : Loss = 143750.13770439874\n",
      "Epoch 11/1000 : Loss = 143749.93365081566\n",
      "Epoch 12/1000 : Loss = 143749.71449470962\n",
      "Epoch 13/1000 : Loss = 143749.47831470674\n",
      "Epoch 14/1000 : Loss = 143749.2230065247\n",
      "Epoch 15/1000 : Loss = 143748.94625995413\n",
      "Epoch 16/1000 : Loss = 143748.64553369425\n",
      "Epoch 17/1000 : Loss = 143748.31802777355\n",
      "Epoch 18/1000 : Loss = 143747.96065326536\n",
      "Epoch 19/1000 : Loss = 143747.56999898184\n",
      "Epoch 20/1000 : Loss = 143747.14229479796\n",
      "Epoch 21/1000 : Loss = 143746.67337122446\n",
      "Epoch 22/1000 : Loss = 143746.15861481312\n",
      "Epoch 23/1000 : Loss = 143745.5929189326\n",
      "Epoch 24/1000 : Loss = 143744.97062941143\n",
      "Epoch 25/1000 : Loss = 143744.28548449225\n",
      "Epoch 26/1000 : Loss = 143743.530548482\n",
      "Epoch 27/1000 : Loss = 143742.69813842853\n",
      "Epoch 28/1000 : Loss = 143741.77974307621\n",
      "Epoch 29/1000 : Loss = 143740.76593328576\n",
      "Epoch 30/1000 : Loss = 143739.646263013\n",
      "Epoch 31/1000 : Loss = 143738.40915985758\n",
      "Epoch 32/1000 : Loss = 143737.04180408068\n",
      "Epoch 33/1000 : Loss = 143735.52999489172\n",
      "Epoch 34/1000 : Loss = 143733.85800267116\n",
      "Epoch 35/1000 : Loss = 143732.00840566785\n",
      "Epoch 36/1000 : Loss = 143729.96190955918\n",
      "Epoch 37/1000 : Loss = 143727.69714809992\n",
      "Epoch 38/1000 : Loss = 143725.19046290749\n",
      "Epoch 39/1000 : Loss = 143722.41566023484\n",
      "Epoch 40/1000 : Loss = 143719.34374236886\n",
      "Epoch 41/1000 : Loss = 143715.94261105507\n",
      "Epoch 42/1000 : Loss = 143712.1767400939\n",
      "Epoch 43/1000 : Loss = 143708.00681397063\n",
      "Epoch 44/1000 : Loss = 143703.38932907375\n",
      "Epoch 45/1000 : Loss = 143698.27615372176\n",
      "Epoch 46/1000 : Loss = 143692.61404284913\n",
      "Epoch 47/1000 : Loss = 143686.34410280705\n",
      "Epoch 48/1000 : Loss = 143679.40120129575\n",
      "Epoch 49/1000 : Loss = 143671.71331697883\n",
      "Epoch 50/1000 : Loss = 143663.20082281512\n",
      "Epoch 51/1000 : Loss = 143653.7756965923\n",
      "Epoch 52/1000 : Loss = 143643.34065154917\n",
      "Epoch 53/1000 : Loss = 143631.78817933312\n",
      "Epoch 54/1000 : Loss = 143618.9994968435\n",
      "Epoch 55/1000 : Loss = 143604.84338777812\n",
      "Epoch 56/1000 : Loss = 143589.1749289123\n",
      "Epoch 57/1000 : Loss = 143571.83409029822\n",
      "Epoch 58/1000 : Loss = 143552.6441976973\n",
      "Epoch 59/1000 : Loss = 143531.4102446252\n",
      "Epoch 60/1000 : Loss = 143507.91704042928\n",
      "Epoch 61/1000 : Loss = 143481.9271798153\n",
      "Epoch 62/1000 : Loss = 143453.17881823095\n",
      "Epoch 63/1000 : Loss = 143421.3832364818\n",
      "Epoch 64/1000 : Loss = 143386.2221769494\n",
      "Epoch 65/1000 : Loss = 143347.34493280022\n",
      "Epoch 66/1000 : Loss = 143304.36517067073\n",
      "Epoch 67/1000 : Loss = 143256.85746650968\n",
      "Epoch 68/1000 : Loss = 143204.35353361466\n",
      "Epoch 69/1000 : Loss = 143146.33812147766\n",
      "Epoch 70/1000 : Loss = 143082.24456392034\n",
      "Epoch 71/1000 : Loss = 143011.4499552618\n",
      "Epoch 72/1000 : Loss = 142933.26993401948\n",
      "Epoch 73/1000 : Loss = 142846.95305504923\n",
      "Epoch 74/1000 : Loss = 142751.6747332346\n",
      "Epoch 75/1000 : Loss = 142646.5307450497\n",
      "Epoch 76/1000 : Loss = 142530.53027876728\n",
      "Epoch 77/1000 : Loss = 142402.58853004972\n",
      "Epoch 78/1000 : Loss = 142261.51884746042\n",
      "Epoch 79/1000 : Loss = 142106.02444244392\n",
      "Epoch 80/1000 : Loss = 141934.68969098083\n",
      "Epoch 81/1000 : Loss = 141745.97106989528\n",
      "Epoch 82/1000 : Loss = 141538.18779025637\n",
      "Epoch 83/1000 : Loss = 141309.51221403747\n",
      "Epoch 84/1000 : Loss = 141057.9601688637\n",
      "Epoch 85/1000 : Loss = 140781.3813099568\n",
      "Epoch 86/1000 : Loss = 140477.4497189855\n",
      "Epoch 87/1000 : Loss = 140143.65497715375\n",
      "Epoch 88/1000 : Loss = 139777.2940050983\n",
      "Epoch 89/1000 : Loss = 139375.4640255388\n",
      "Epoch 90/1000 : Loss = 138935.05707638603\n",
      "Epoch 91/1000 : Loss = 138452.75658207844\n",
      "Epoch 92/1000 : Loss = 137925.03657875754\n",
      "Epoch 93/1000 : Loss = 137348.1642832239\n",
      "Epoch 94/1000 : Loss = 136718.20679433926\n",
      "Epoch 95/1000 : Loss = 136031.04281531542\n",
      "Epoch 96/1000 : Loss = 135282.38038140512\n",
      "Epoch 97/1000 : Loss = 134467.78166332125\n",
      "Epoch 98/1000 : Loss = 133582.69598362612\n",
      "Epoch 99/1000 : Loss = 132622.50222032692\n",
      "Epoch 100/1000 : Loss = 131582.56176544938\n",
      "Epoch 101/1000 : Loss = 130458.28314031835\n",
      "Epoch 102/1000 : Loss = 129245.19922524216\n",
      "Epoch 103/1000 : Loss = 127939.05781921736\n",
      "Epoch 104/1000 : Loss = 126535.92588453872\n",
      "Epoch 105/1000 : Loss = 125032.30733253939\n",
      "Epoch 106/1000 : Loss = 123425.27355463148\n",
      "Epoch 107/1000 : Loss = 121712.60508913363\n",
      "Epoch 108/1000 : Loss = 119892.94184216394\n",
      "Epoch 109/1000 : Loss = 117965.93816926492\n",
      "Epoch 110/1000 : Loss = 115932.41791338866\n",
      "Epoch 111/1000 : Loss = 113794.52324939739\n",
      "Epoch 112/1000 : Loss = 111555.84999808604\n",
      "Epoch 113/1000 : Loss = 109221.56106421069\n",
      "Epoch 114/1000 : Loss = 106798.46896679426\n",
      "Epoch 115/1000 : Loss = 104295.07822347658\n",
      "Epoch 116/1000 : Loss = 101721.57877939833\n",
      "Epoch 117/1000 : Loss = 99089.78286712684\n",
      "Epoch 118/1000 : Loss = 96412.9997301493\n",
      "Epoch 119/1000 : Loss = 93705.84554468209\n",
      "Epoch 120/1000 : Loss = 90983.98953898341\n",
      "Epoch 121/1000 : Loss = 88263.84152745327\n",
      "Epoch 122/1000 : Loss = 85562.19052630811\n",
      "Epoch 123/1000 : Loss = 82895.80838312932\n",
      "Epoch 124/1000 : Loss = 80281.03596728345\n",
      "Epoch 125/1000 : Loss = 77733.3719736463\n",
      "Epoch 126/1000 : Loss = 75267.08540798302\n",
      "Epoch 127/1000 : Loss = 72894.87211443171\n",
      "Epoch 128/1000 : Loss = 70627.57323688951\n",
      "Epoch 129/1000 : Loss = 68473.9694597524\n",
      "Epoch 130/1000 : Loss = 66440.65963841115\n",
      "Epoch 131/1000 : Loss = 64532.02655048293\n",
      "Epoch 132/1000 : Loss = 62750.28659574324\n",
      "Epoch 133/1000 : Loss = 61095.614951270436\n",
      "Epoch 134/1000 : Loss = 59566.33345089013\n",
      "Epoch 135/1000 : Loss = 58159.14563894574\n",
      "Epoch 136/1000 : Loss = 56869.40218128745\n",
      "Epoch 137/1000 : Loss = 55691.380037242896\n",
      "Epoch 138/1000 : Loss = 54618.5602782526\n",
      "Epoch 139/1000 : Loss = 53643.891846868275\n",
      "Epoch 140/1000 : Loss = 52760.03150342617\n",
      "Epoch 141/1000 : Loss = 51959.55333851208\n",
      "Epoch 142/1000 : Loss = 51235.12422500411\n",
      "Epoch 143/1000 : Loss = 50579.64421383483\n",
      "Epoch 144/1000 : Loss = 49986.353002899734\n",
      "Epoch 145/1000 : Loss = 49448.905173176856\n",
      "Epoch 146/1000 : Loss = 48961.417902911664\n",
      "Epoch 147/1000 : Loss = 48518.49540061312\n",
      "Epoch 148/1000 : Loss = 48115.234429331365\n",
      "Epoch 149/1000 : Loss = 47747.21512719223\n",
      "Epoch 150/1000 : Loss = 47410.4809583613\n",
      "Epoch 151/1000 : Loss = 47101.51113877616\n",
      "Epoch 152/1000 : Loss = 46817.18834032005\n",
      "Epoch 153/1000 : Loss = 46554.76393658481\n",
      "Epoch 154/1000 : Loss = 46311.822547777636\n",
      "Epoch 155/1000 : Loss = 46086.24719266097\n",
      "Epoch 156/1000 : Loss = 45876.185971766405\n",
      "Epoch 157/1000 : Loss = 45680.02089059658\n",
      "Epoch 158/1000 : Loss = 45496.339180850875\n",
      "Epoch 159/1000 : Loss = 45323.90728537926\n",
      "Epoch 160/1000 : Loss = 45161.647530472066\n",
      "Epoch 161/1000 : Loss = 45008.617408706195\n",
      "Epoch 162/1000 : Loss = 44863.99132868567\n",
      "Epoch 163/1000 : Loss = 44727.04464723001\n",
      "Epoch 164/1000 : Loss = 44597.13977847551\n",
      "Epoch 165/1000 : Loss = 44473.71416763338\n",
      "Epoch 166/1000 : Loss = 44356.26992048713\n",
      "Epoch 167/1000 : Loss = 44244.36488973688\n",
      "Epoch 168/1000 : Loss = 44137.60503343214\n",
      "Epoch 169/1000 : Loss = 44035.637877066496\n",
      "Epoch 170/1000 : Loss = 43938.14692806565\n",
      "Epoch 171/1000 : Loss = 43844.84690843353\n",
      "Epoch 172/1000 : Loss = 43755.479687604864\n",
      "Epoch 173/1000 : Loss = 43669.81081270564\n",
      "Epoch 174/1000 : Loss = 43587.626547237516\n",
      "Epoch 175/1000 : Loss = 43508.73134160212\n",
      "Epoch 176/1000 : Loss = 43432.945669867455\n",
      "Epoch 177/1000 : Loss = 43360.10417681917\n",
      "Epoch 178/1000 : Loss = 43290.054087718956\n",
      "Epoch 179/1000 : Loss = 43222.65384043495\n",
      "Epoch 180/1000 : Loss = 43157.771905823814\n",
      "Epoch 181/1000 : Loss = 43095.28576755448\n",
      "Epoch 182/1000 : Loss = 43035.081037083786\n",
      "Epoch 183/1000 : Loss = 42977.05068332229\n",
      "Epoch 184/1000 : Loss = 42921.094359769064\n",
      "Epoch 185/1000 : Loss = 42867.11781462367\n",
      "Epoch 186/1000 : Loss = 42815.03237168198\n",
      "Epoch 187/1000 : Loss = 42764.75447175348\n",
      "Epoch 188/1000 : Loss = 42716.205265957964\n",
      "Epoch 189/1000 : Loss = 42669.31025361815\n",
      "Epoch 190/1000 : Loss = 42623.99895860285\n",
      "Epoch 191/1000 : Loss = 42580.20463892909\n",
      "Epoch 192/1000 : Loss = 42537.86402522929\n",
      "Epoch 193/1000 : Loss = 42496.917084359084\n",
      "Epoch 194/1000 : Loss = 42457.30680498183\n",
      "Epoch 195/1000 : Loss = 42418.9790024347\n",
      "Epoch 196/1000 : Loss = 42381.88214057705\n",
      "Epoch 197/1000 : Loss = 42345.967168651805\n",
      "Epoch 198/1000 : Loss = 42311.187371470245\n",
      "Epoch 199/1000 : Loss = 42277.49823146442\n",
      "Epoch 200/1000 : Loss = 42244.857301350195\n",
      "Epoch 201/1000 : Loss = 42213.22408631119\n",
      "Epoch 202/1000 : Loss = 42182.55993475549\n",
      "Epoch 203/1000 : Loss = 42152.827936818656\n",
      "Epoch 204/1000 : Loss = 42123.99282988669\n",
      "Epoch 205/1000 : Loss = 42096.02091050363\n",
      "Epoch 206/1000 : Loss = 42068.87995209973\n",
      "Epoch 207/1000 : Loss = 42042.53912804373\n",
      "Epoch 208/1000 : Loss = 42016.96893957564\n",
      "Epoch 209/1000 : Loss = 41992.14114822555\n",
      "Epoch 210/1000 : Loss = 41968.0287123657\n",
      "Epoch 211/1000 : Loss = 41944.605727577444\n",
      "Epoch 212/1000 : Loss = 41921.84737054876\n",
      "Epoch 213/1000 : Loss = 41899.72984624273\n",
      "Epoch 214/1000 : Loss = 41878.230338103516\n",
      "Epoch 215/1000 : Loss = 41857.326961087456\n",
      "Epoch 216/1000 : Loss = 41836.998717324386\n",
      "Epoch 217/1000 : Loss = 41817.225454233034\n",
      "Epoch 218/1000 : Loss = 41797.98782492759\n",
      "Epoch 219/1000 : Loss = 41779.267250766905\n",
      "Epoch 220/1000 : Loss = 41761.04588590911\n",
      "Epoch 221/1000 : Loss = 41743.30658374525\n",
      "Epoch 222/1000 : Loss = 41726.032865096284\n",
      "Epoch 223/1000 : Loss = 41709.20888806392\n",
      "Epoch 224/1000 : Loss = 41692.81941943698\n",
      "Epoch 225/1000 : Loss = 41676.84980755981\n",
      "Epoch 226/1000 : Loss = 41661.285956576634\n",
      "Epoch 227/1000 : Loss = 41646.11430197181\n",
      "Epoch 228/1000 : Loss = 41631.321787331464\n",
      "Epoch 229/1000 : Loss = 41616.89584225636\n",
      "Epoch 230/1000 : Loss = 41602.824361361636\n",
      "Epoch 231/1000 : Loss = 41589.09568430244\n",
      "Epoch 232/1000 : Loss = 41575.69857676786\n",
      "Epoch 233/1000 : Loss = 41562.62221239149\n",
      "Epoch 234/1000 : Loss = 41549.85615552723\n",
      "Epoch 235/1000 : Loss = 41537.390344844396\n",
      "Epoch 236/1000 : Loss = 41525.21507769798\n",
      "Epoch 237/1000 : Loss = 41513.320995232956\n",
      "Epoch 238/1000 : Loss = 41501.69906818395\n",
      "Epoch 239/1000 : Loss = 41490.34058333326\n",
      "Epoch 240/1000 : Loss = 41479.237130593814\n",
      "Epoch 241/1000 : Loss = 41468.38059068444\n",
      "Epoch 242/1000 : Loss = 41457.76312336668\n",
      "Epoch 243/1000 : Loss = 41447.37715621491\n",
      "Epoch 244/1000 : Loss = 41437.21537389287\n",
      "Epoch 245/1000 : Loss = 41427.27070791065\n",
      "Epoch 246/1000 : Loss = 41417.53632683837\n",
      "Epoch 247/1000 : Loss = 41408.005626953935\n",
      "Epoch 248/1000 : Loss = 41398.67222330312\n",
      "Epoch 249/1000 : Loss = 41389.52994115204\n",
      "Epoch 250/1000 : Loss = 41380.572807812496\n",
      "Epoch 251/1000 : Loss = 41371.795044822225\n",
      "Epoch 252/1000 : Loss = 41363.19106046325\n",
      "Epoch 253/1000 : Loss = 41354.75544260132\n",
      "Epoch 254/1000 : Loss = 41346.48295183178\n",
      "Epoch 255/1000 : Loss = 41338.36851491716\n",
      "Epoch 256/1000 : Loss = 41330.407218502194\n",
      "Epoch 257/1000 : Loss = 41322.59430309417\n",
      "Epoch 258/1000 : Loss = 41314.92515729512\n",
      "Epoch 259/1000 : Loss = 41307.39531227501\n",
      "Epoch 260/1000 : Loss = 41300.00043647424\n",
      "Epoch 261/1000 : Loss = 41292.73633052523\n",
      "Epoch 262/1000 : Loss = 41285.59892238265\n",
      "Epoch 263/1000 : Loss = 41278.584262653145\n",
      "Epoch 264/1000 : Loss = 41271.68852011553\n",
      "Epoch 265/1000 : Loss = 41264.90797742244\n",
      "Epoch 266/1000 : Loss = 41258.2390269755\n",
      "Epoch 267/1000 : Loss = 41251.67816696683\n",
      "Epoch 268/1000 : Loss = 41245.22199757806\n",
      "Epoch 269/1000 : Loss = 41238.867217331644\n",
      "Epoch 270/1000 : Loss = 41232.61061958637\n",
      "Epoch 271/1000 : Loss = 41226.449089171314\n",
      "Epoch 272/1000 : Loss = 41220.37959915236\n",
      "Epoch 273/1000 : Loss = 41214.39920772501\n",
      "Epoch 274/1000 : Loss = 41208.50505522865\n",
      "Epoch 275/1000 : Loss = 41202.6943612764\n",
      "Epoch 276/1000 : Loss = 41196.96442199595\n",
      "Epoch 277/1000 : Loss = 41191.312607376916\n",
      "Epoch 278/1000 : Loss = 41185.736358719485\n",
      "Epoch 279/1000 : Loss = 41180.23318618076\n",
      "Epoch 280/1000 : Loss = 41174.80066641437\n",
      "Epoch 281/1000 : Loss = 41169.436440299\n",
      "Epoch 282/1000 : Loss = 41164.13821075373\n",
      "Epoch 283/1000 : Loss = 41158.90374063421\n",
      "Epoch 284/1000 : Loss = 41153.730850708314\n",
      "Epoch 285/1000 : Loss = 41148.61741770738\n",
      "Epoch 286/1000 : Loss = 41143.56137244927\n",
      "Epoch 287/1000 : Loss = 41138.560698031564\n",
      "Epoch 288/1000 : Loss = 41133.61342809078\n",
      "Epoch 289/1000 : Loss = 41128.71764512622\n",
      "Epoch 290/1000 : Loss = 41123.87147888461\n",
      "Epoch 291/1000 : Loss = 41119.07310480405\n",
      "Epoch 292/1000 : Loss = 41114.32074251448\n",
      "Epoch 293/1000 : Loss = 41109.61265439228\n",
      "Epoch 294/1000 : Loss = 41104.94714416749\n",
      "Epoch 295/1000 : Loss = 41100.32255558092\n",
      "Epoch 296/1000 : Loss = 41095.73727108947\n",
      "Epoch 297/1000 : Loss = 41091.18971061793\n",
      "Epoch 298/1000 : Loss = 41086.678330355\n",
      "Epoch 299/1000 : Loss = 41082.20162159266\n",
      "Epoch 300/1000 : Loss = 41077.75810960627\n",
      "Epoch 301/1000 : Loss = 41073.34635257435\n",
      "Epoch 302/1000 : Loss = 41068.96494053671\n",
      "Epoch 303/1000 : Loss = 41064.61249438899\n",
      "Epoch 304/1000 : Loss = 41060.287664912576\n",
      "Epoch 305/1000 : Loss = 41055.9891318385\n",
      "Epoch 306/1000 : Loss = 41051.715602944074\n",
      "Epoch 307/1000 : Loss = 41047.46581318091\n",
      "Epoch 308/1000 : Loss = 41043.23852383334\n",
      "Epoch 309/1000 : Loss = 41039.032521705994\n",
      "Epoch 310/1000 : Loss = 41034.8466183396\n",
      "Epoch 311/1000 : Loss = 41030.67964925386\n",
      "Epoch 312/1000 : Loss = 41026.53047321655\n",
      "Epoch 313/1000 : Loss = 41022.39797153754\n",
      "Epoch 314/1000 : Loss = 41018.28104738752\n",
      "Epoch 315/1000 : Loss = 41014.178625139815\n",
      "Epoch 316/1000 : Loss = 41010.089649734895\n",
      "Epoch 317/1000 : Loss = 41006.01308606697\n",
      "Epoch 318/1000 : Loss = 41001.947918391044\n",
      "Epoch 319/1000 : Loss = 40997.89314975087\n",
      "Epoch 320/1000 : Loss = 40993.847801425996\n",
      "Epoch 321/1000 : Loss = 40989.81091239823\n",
      "Epoch 322/1000 : Loss = 40985.78153883594\n",
      "Epoch 323/1000 : Loss = 40981.758753596034\n",
      "Epoch 324/1000 : Loss = 40977.741645743576\n",
      "Epoch 325/1000 : Loss = 40973.72932008678\n",
      "Epoch 326/1000 : Loss = 40969.72089672912\n",
      "Epoch 327/1000 : Loss = 40965.71551063589\n",
      "Epoch 328/1000 : Loss = 40961.7123112161\n",
      "Epoch 329/1000 : Loss = 40957.710461918614\n",
      "Epoch 330/1000 : Loss = 40953.70913984233\n",
      "Epoch 331/1000 : Loss = 40949.70753535944\n",
      "Epoch 332/1000 : Loss = 40945.70485175218\n",
      "Epoch 333/1000 : Loss = 40941.70030486181\n",
      "Epoch 334/1000 : Loss = 40937.693122749944\n",
      "Epoch 335/1000 : Loss = 40933.68254537163\n",
      "Epoch 336/1000 : Loss = 40929.66782425975\n",
      "Epoch 337/1000 : Loss = 40925.64822222081\n",
      "Epoch 338/1000 : Loss = 40921.62301304094\n",
      "Epoch 339/1000 : Loss = 40917.59148120269\n",
      "Epoch 340/1000 : Loss = 40913.552921611656\n",
      "Epoch 341/1000 : Loss = 40909.5066393329\n",
      "Epoch 342/1000 : Loss = 40905.45194933691\n",
      "Epoch 343/1000 : Loss = 40901.388176254404\n",
      "Epoch 344/1000 : Loss = 40897.31465414055\n",
      "Epoch 345/1000 : Loss = 40893.23072624713\n",
      "Epoch 346/1000 : Loss = 40889.13574480357\n",
      "Epoch 347/1000 : Loss = 40885.029070805875\n",
      "Epoch 348/1000 : Loss = 40880.910073813364\n",
      "Epoch 349/1000 : Loss = 40876.77813175318\n",
      "Epoch 350/1000 : Loss = 40872.632630732085\n",
      "Epoch 351/1000 : Loss = 40868.472964855326\n",
      "Epoch 352/1000 : Loss = 40864.298536052986\n",
      "Epoch 353/1000 : Loss = 40860.10875391234\n",
      "Epoch 354/1000 : Loss = 40855.90303551748\n",
      "Epoch 355/1000 : Loss = 40851.680805295\n",
      "Epoch 356/1000 : Loss = 40847.441494865736\n",
      "Epoch 357/1000 : Loss = 40843.1845429032\n",
      "Epoch 358/1000 : Loss = 40838.90939499696\n",
      "Epoch 359/1000 : Loss = 40834.61550352249\n",
      "Epoch 360/1000 : Loss = 40830.30232751612\n",
      "Epoch 361/1000 : Loss = 40825.96933255566\n",
      "Epoch 362/1000 : Loss = 40821.61599064601\n",
      "Epoch 363/1000 : Loss = 40817.24178011002\n",
      "Epoch 364/1000 : Loss = 40812.846185484246\n",
      "Epoch 365/1000 : Loss = 40808.42869741966\n",
      "Epoch 366/1000 : Loss = 40803.988812586766\n",
      "Epoch 367/1000 : Loss = 40799.52603358558\n",
      "Epoch 368/1000 : Loss = 40795.03986885993\n",
      "Epoch 369/1000 : Loss = 40790.52983261589\n",
      "Epoch 370/1000 : Loss = 40785.99544474491\n",
      "Epoch 371/1000 : Loss = 40781.43623075042\n",
      "Epoch 372/1000 : Loss = 40776.85172167883\n",
      "Epoch 373/1000 : Loss = 40772.24145405434\n",
      "Epoch 374/1000 : Loss = 40767.60496981748\n",
      "Epoch 375/1000 : Loss = 40762.941816267055\n",
      "Epoch 376/1000 : Loss = 40758.25154600639\n",
      "Epoch 377/1000 : Loss = 40753.53371689213\n",
      "Epoch 378/1000 : Loss = 40748.787891987005\n",
      "Epoch 379/1000 : Loss = 40744.013639516015\n",
      "Epoch 380/1000 : Loss = 40739.21053282508\n",
      "Epoch 381/1000 : Loss = 40734.37815034364\n",
      "Epoch 382/1000 : Loss = 40729.51607554986\n",
      "Epoch 383/1000 : Loss = 40724.62389693866\n",
      "Epoch 384/1000 : Loss = 40719.70120799314\n",
      "Epoch 385/1000 : Loss = 40714.74760715813\n",
      "Epoch 386/1000 : Loss = 40709.76269781702\n",
      "Epoch 387/1000 : Loss = 40704.74608827084\n",
      "Epoch 388/1000 : Loss = 40699.697391720176\n",
      "Epoch 389/1000 : Loss = 40694.616226249345\n",
      "Epoch 390/1000 : Loss = 40689.50221481314\n",
      "Epoch 391/1000 : Loss = 40684.35498522595\n",
      "Epoch 392/1000 : Loss = 40679.17417015308\n",
      "Epoch 393/1000 : Loss = 40673.95940710418\n",
      "Epoch 394/1000 : Loss = 40668.71033842913\n",
      "Epoch 395/1000 : Loss = 40663.42661131564\n",
      "Epoch 396/1000 : Loss = 40658.10787778927\n",
      "Epoch 397/1000 : Loss = 40652.75379471495\n",
      "Epoch 398/1000 : Loss = 40647.36402380081\n",
      "Epoch 399/1000 : Loss = 40641.938231603584\n",
      "Epoch 400/1000 : Loss = 40636.47608953594\n",
      "Epoch 401/1000 : Loss = 40630.97727387534\n",
      "Epoch 402/1000 : Loss = 40625.441465774806\n",
      "Epoch 403/1000 : Loss = 40619.868351274956\n",
      "Epoch 404/1000 : Loss = 40614.25762131802\n",
      "Epoch 405/1000 : Loss = 40608.6089717627\n",
      "Epoch 406/1000 : Loss = 40602.92210340134\n",
      "Epoch 407/1000 : Loss = 40597.19672197743\n",
      "Epoch 408/1000 : Loss = 40591.432538205416\n",
      "Epoch 409/1000 : Loss = 40585.62926779093\n",
      "Epoch 410/1000 : Loss = 40579.7866314529\n",
      "Epoch 411/1000 : Loss = 40573.9043549463\n",
      "Epoch 412/1000 : Loss = 40567.98216908631\n",
      "Epoch 413/1000 : Loss = 40562.01980977345\n",
      "Epoch 414/1000 : Loss = 40556.01701801954\n",
      "Epoch 415/1000 : Loss = 40549.97353997498\n",
      "Epoch 416/1000 : Loss = 40543.88912695641\n",
      "Epoch 417/1000 : Loss = 40537.76353547571\n",
      "Epoch 418/1000 : Loss = 40531.596527269394\n",
      "Epoch 419/1000 : Loss = 40525.38786932891\n",
      "Epoch 420/1000 : Loss = 40519.13733393172\n",
      "Epoch 421/1000 : Loss = 40512.84469867284\n",
      "Epoch 422/1000 : Loss = 40506.509746496995\n",
      "Epoch 423/1000 : Loss = 40500.132265731474\n",
      "Epoch 424/1000 : Loss = 40493.7120501191\n",
      "Epoch 425/1000 : Loss = 40487.24889885221\n",
      "Epoch 426/1000 : Loss = 40480.742616606425\n",
      "Epoch 427/1000 : Loss = 40474.19301357511\n",
      "Epoch 428/1000 : Loss = 40467.599905504074\n",
      "Epoch 429/1000 : Loss = 40460.96311372644\n",
      "Epoch 430/1000 : Loss = 40454.28246519773\n",
      "Epoch 431/1000 : Loss = 40447.55779253113\n",
      "Epoch 432/1000 : Loss = 40440.78893403293\n",
      "Epoch 433/1000 : Loss = 40433.975733737636\n",
      "Epoch 434/1000 : Loss = 40427.118041443755\n",
      "Epoch 435/1000 : Loss = 40420.21571274875\n",
      "Epoch 436/1000 : Loss = 40413.26860908467\n",
      "Epoch 437/1000 : Loss = 40406.27659775297\n",
      "Epoch 438/1000 : Loss = 40399.239551959705\n",
      "Epoch 439/1000 : Loss = 40392.157350850044\n",
      "Epoch 440/1000 : Loss = 40385.02987954285\n",
      "Epoch 441/1000 : Loss = 40377.857029164676\n",
      "Epoch 442/1000 : Loss = 40370.6386968837\n",
      "Epoch 443/1000 : Loss = 40363.37478594289\n",
      "Epoch 444/1000 : Loss = 40356.06520569302\n",
      "Epoch 445/1000 : Loss = 40348.70987162501\n",
      "Epoch 446/1000 : Loss = 40341.30870540184\n",
      "Epoch 447/1000 : Loss = 40333.86163488967\n",
      "Epoch 448/1000 : Loss = 40326.36859418871\n",
      "Epoch 449/1000 : Loss = 40318.82952366305\n",
      "Epoch 450/1000 : Loss = 40311.24436996992\n",
      "Epoch 451/1000 : Loss = 40303.61308608822\n",
      "Epoch 452/1000 : Loss = 40295.93563134629\n",
      "Epoch 453/1000 : Loss = 40288.21197144874\n",
      "Epoch 454/1000 : Loss = 40280.44207850234\n",
      "Epoch 455/1000 : Loss = 40272.62593104135\n",
      "Epoch 456/1000 : Loss = 40264.76351405143\n",
      "Epoch 457/1000 : Loss = 40256.85481899281\n",
      "Epoch 458/1000 : Loss = 40248.89984382254\n",
      "Epoch 459/1000 : Loss = 40240.89859301537\n",
      "Epoch 460/1000 : Loss = 40232.85107758371\n",
      "Epoch 461/1000 : Loss = 40224.75731509637\n",
      "Epoch 462/1000 : Loss = 40216.61732969633\n",
      "Epoch 463/1000 : Loss = 40208.431152116886\n",
      "Epoch 464/1000 : Loss = 40200.198819696976\n",
      "Epoch 465/1000 : Loss = 40191.92037639495\n",
      "Epoch 466/1000 : Loss = 40183.59587280108\n",
      "Epoch 467/1000 : Loss = 40175.22536614893\n",
      "Epoch 468/1000 : Loss = 40166.808920324875\n",
      "Epoch 469/1000 : Loss = 40158.34660587668\n",
      "Epoch 470/1000 : Loss = 40149.83850002054\n",
      "Epoch 471/1000 : Loss = 40141.28468664645\n",
      "Epoch 472/1000 : Loss = 40132.6852563222\n",
      "Epoch 473/1000 : Loss = 40124.04030629579\n",
      "Epoch 474/1000 : Loss = 40115.34994049655\n",
      "Epoch 475/1000 : Loss = 40106.61426953433\n",
      "Epoch 476/1000 : Loss = 40097.833410697305\n",
      "Epoch 477/1000 : Loss = 40089.007487948016\n",
      "Epoch 478/1000 : Loss = 40080.136631917856\n",
      "Epoch 479/1000 : Loss = 40071.22097990007\n",
      "Epoch 480/1000 : Loss = 40062.26067584061\n",
      "Epoch 481/1000 : Loss = 40053.25587032757\n",
      "Epoch 482/1000 : Loss = 40044.20672057913\n",
      "Epoch 483/1000 : Loss = 40035.113390429084\n",
      "Epoch 484/1000 : Loss = 40025.97605031142\n",
      "Epoch 485/1000 : Loss = 40016.79487724245\n",
      "Epoch 486/1000 : Loss = 40007.57005480151\n",
      "Epoch 487/1000 : Loss = 39998.30177310975\n",
      "Epoch 488/1000 : Loss = 39988.99022880722\n",
      "Epoch 489/1000 : Loss = 39979.635625027826\n",
      "Epoch 490/1000 : Loss = 39970.238171372905\n",
      "Epoch 491/1000 : Loss = 39960.79808388262\n",
      "Epoch 492/1000 : Loss = 39951.31558500557\n",
      "Epoch 493/1000 : Loss = 39941.79090356654\n",
      "Epoch 494/1000 : Loss = 39932.224274732755\n",
      "Epoch 495/1000 : Loss = 39922.61593997735\n",
      "Epoch 496/1000 : Loss = 39912.96614704229\n",
      "Epoch 497/1000 : Loss = 39903.275149898036\n",
      "Epoch 498/1000 : Loss = 39893.54320870262\n",
      "Epoch 499/1000 : Loss = 39883.77058975771\n",
      "Epoch 500/1000 : Loss = 39873.95756546396\n",
      "Epoch 501/1000 : Loss = 39864.10441427332\n",
      "Epoch 502/1000 : Loss = 39854.211420640495\n",
      "Epoch 503/1000 : Loss = 39844.27887497201\n",
      "Epoch 504/1000 : Loss = 39834.30707357359\n",
      "Epoch 505/1000 : Loss = 39824.296318595756\n",
      "Epoch 506/1000 : Loss = 39814.246917977565\n",
      "Epoch 507/1000 : Loss = 39804.15918538858\n",
      "Epoch 508/1000 : Loss = 39794.0334401688\n",
      "Epoch 509/1000 : Loss = 39783.87000726752\n",
      "Epoch 510/1000 : Loss = 39773.66921717944\n",
      "Epoch 511/1000 : Loss = 39763.431405879826\n",
      "Epoch 512/1000 : Loss = 39753.156914757725\n",
      "Epoch 513/1000 : Loss = 39742.846090547275\n",
      "Epoch 514/1000 : Loss = 39732.49928525759\n",
      "Epoch 515/1000 : Loss = 39722.11685610084\n",
      "Epoch 516/1000 : Loss = 39711.699165418824\n",
      "Epoch 517/1000 : Loss = 39701.24658060772\n",
      "Epoch 518/1000 : Loss = 39690.7594740415\n",
      "Epoch 519/1000 : Loss = 39680.23822299376\n",
      "Epoch 520/1000 : Loss = 39669.68320955769\n",
      "Epoch 521/1000 : Loss = 39659.094820564984\n",
      "Epoch 522/1000 : Loss = 39648.47344750303\n",
      "Epoch 523/1000 : Loss = 39637.81948643074\n",
      "Epoch 524/1000 : Loss = 39627.13333789294\n",
      "Epoch 525/1000 : Loss = 39616.415406833396\n",
      "Epoch 526/1000 : Loss = 39605.666102506555\n",
      "Epoch 527/1000 : Loss = 39594.88583838791\n",
      "Epoch 528/1000 : Loss = 39584.07503208311\n",
      "Epoch 529/1000 : Loss = 39573.2341052359\n",
      "Epoch 530/1000 : Loss = 39562.36348343482\n",
      "Epoch 531/1000 : Loss = 39551.46359611871\n",
      "Epoch 532/1000 : Loss = 39540.534876481375\n",
      "Epoch 533/1000 : Loss = 39529.577761374676\n",
      "Epoch 534/1000 : Loss = 39518.59269121104\n",
      "Epoch 535/1000 : Loss = 39507.58010986481\n",
      "Epoch 536/1000 : Loss = 39496.54046457279\n",
      "Epoch 537/1000 : Loss = 39485.47420583353\n",
      "Epoch 538/1000 : Loss = 39474.381787306134\n",
      "Epoch 539/1000 : Loss = 39463.26366570818\n",
      "Epoch 540/1000 : Loss = 39452.12030071275\n",
      "Epoch 541/1000 : Loss = 39440.952154844774\n",
      "Epoch 542/1000 : Loss = 39429.7596933768\n",
      "Epoch 543/1000 : Loss = 39418.54338422421\n",
      "Epoch 544/1000 : Loss = 39407.30369783934\n",
      "Epoch 545/1000 : Loss = 39396.04110710596\n",
      "Epoch 546/1000 : Loss = 39384.75608723243\n",
      "Epoch 547/1000 : Loss = 39373.44911564495\n",
      "Epoch 548/1000 : Loss = 39362.12067188007\n",
      "Epoch 549/1000 : Loss = 39350.77123747727\n",
      "Epoch 550/1000 : Loss = 39339.401295870935\n",
      "Epoch 551/1000 : Loss = 39328.01133228208\n",
      "Epoch 552/1000 : Loss = 39316.60183361023\n",
      "Epoch 553/1000 : Loss = 39305.17328832477\n",
      "Epoch 554/1000 : Loss = 39293.726186356354\n",
      "Epoch 555/1000 : Loss = 39282.261018988545\n",
      "Epoch 556/1000 : Loss = 39270.77827874884\n",
      "Epoch 557/1000 : Loss = 39259.27845930054\n",
      "Epoch 558/1000 : Loss = 39247.762055334075\n",
      "Epoch 559/1000 : Loss = 39236.22956245883\n",
      "Epoch 560/1000 : Loss = 39224.68147709507\n",
      "Epoch 561/1000 : Loss = 39213.11829636627\n",
      "Epoch 562/1000 : Loss = 39201.54051799151\n",
      "Epoch 563/1000 : Loss = 39189.94864017838\n",
      "Epoch 564/1000 : Loss = 39178.34316151633\n",
      "Epoch 565/1000 : Loss = 39166.724580870374\n",
      "Epoch 566/1000 : Loss = 39155.093397275225\n",
      "Epoch 567/1000 : Loss = 39143.45010983033\n",
      "Epoch 568/1000 : Loss = 39131.79521759492\n",
      "Epoch 569/1000 : Loss = 39120.129219484224\n",
      "Epoch 570/1000 : Loss = 39108.45261416614\n",
      "Epoch 571/1000 : Loss = 39096.76589995858\n",
      "Epoch 572/1000 : Loss = 39085.069574727684\n",
      "Epoch 573/1000 : Loss = 39073.36413578679\n",
      "Epoch 574/1000 : Loss = 39061.65007979635\n",
      "Epoch 575/1000 : Loss = 39049.92790266455\n",
      "Epoch 576/1000 : Loss = 39038.19809944921\n",
      "Epoch 577/1000 : Loss = 39026.461164260065\n",
      "Epoch 578/1000 : Loss = 39014.717590162945\n",
      "Epoch 579/1000 : Loss = 39002.96786908393\n",
      "Epoch 580/1000 : Loss = 38991.2124917156\n",
      "Epoch 581/1000 : Loss = 38979.45194742398\n",
      "Epoch 582/1000 : Loss = 38967.686724156156\n",
      "Epoch 583/1000 : Loss = 38955.91730835023\n",
      "Epoch 584/1000 : Loss = 38944.14418484538\n",
      "Epoch 585/1000 : Loss = 38932.36783679392\n",
      "Epoch 586/1000 : Loss = 38920.58874557418\n",
      "Epoch 587/1000 : Loss = 38908.80739070521\n",
      "Epoch 588/1000 : Loss = 38897.02424976226\n",
      "Epoch 589/1000 : Loss = 38885.23979829417\n",
      "Epoch 590/1000 : Loss = 38873.45450974184\n",
      "Epoch 591/1000 : Loss = 38861.66885535825\n",
      "Epoch 592/1000 : Loss = 38849.88330412987\n",
      "Epoch 593/1000 : Loss = 38838.09832269994\n",
      "Epoch 594/1000 : Loss = 38826.314375292444\n",
      "Epoch 595/1000 : Loss = 38814.53192363853\n",
      "Epoch 596/1000 : Loss = 38802.75142690397\n",
      "Epoch 597/1000 : Loss = 38790.97334161812\n",
      "Epoch 598/1000 : Loss = 38779.198121604946\n",
      "Epoch 599/1000 : Loss = 38767.42621791507\n",
      "Epoch 600/1000 : Loss = 38755.65807876\n",
      "Epoch 601/1000 : Loss = 38743.894149447566\n",
      "Epoch 602/1000 : Loss = 38732.13487231924\n",
      "Epoch 603/1000 : Loss = 38720.38068668901\n",
      "Epoch 604/1000 : Loss = 38708.63202878415\n",
      "Epoch 605/1000 : Loss = 38696.8893316872\n",
      "Epoch 606/1000 : Loss = 38685.153025280444\n",
      "Epoch 607/1000 : Loss = 38673.423536191134\n",
      "Epoch 608/1000 : Loss = 38661.70128773922\n",
      "Epoch 609/1000 : Loss = 38649.986699886336\n",
      "Epoch 610/1000 : Loss = 38638.28018918679\n",
      "Epoch 611/1000 : Loss = 38626.58216874004\n",
      "Epoch 612/1000 : Loss = 38614.89304814507\n",
      "Epoch 613/1000 : Loss = 38603.213233456474\n",
      "Epoch 614/1000 : Loss = 38591.543127142235\n",
      "Epoch 615/1000 : Loss = 38579.88312804325\n",
      "Epoch 616/1000 : Loss = 38568.233631334544\n",
      "Epoch 617/1000 : Loss = 38556.59502848834\n",
      "Epoch 618/1000 : Loss = 38544.96770723868\n",
      "Epoch 619/1000 : Loss = 38533.35205154795\n",
      "Epoch 620/1000 : Loss = 38521.74844157496\n",
      "Epoch 621/1000 : Loss = 38510.15725364479\n",
      "Epoch 622/1000 : Loss = 38498.57886022044\n",
      "Epoch 623/1000 : Loss = 38487.013629875866\n",
      "Epoch 624/1000 : Loss = 38475.46192727119\n",
      "Epoch 625/1000 : Loss = 38463.924113129004\n",
      "Epoch 626/1000 : Loss = 38452.40054421293\n",
      "Epoch 627/1000 : Loss = 38440.8915733072\n",
      "Epoch 628/1000 : Loss = 38429.39754919845\n",
      "Epoch 629/1000 : Loss = 38417.91881665877\n",
      "Epoch 630/1000 : Loss = 38406.45571643044\n",
      "Epoch 631/1000 : Loss = 38395.008585212476\n",
      "Epoch 632/1000 : Loss = 38383.57775564815\n",
      "Epoch 633/1000 : Loss = 38372.163556315034\n",
      "Epoch 634/1000 : Loss = 38360.76631171552\n",
      "Epoch 635/1000 : Loss = 38349.3863422698\n",
      "Epoch 636/1000 : Loss = 38338.02396430953\n",
      "Epoch 637/1000 : Loss = 38326.67949007363\n",
      "Epoch 638/1000 : Loss = 38315.35322770524\n",
      "Epoch 639/1000 : Loss = 38304.045481250054\n",
      "Epoch 640/1000 : Loss = 38292.75655065629\n",
      "Epoch 641/1000 : Loss = 38281.48673177597\n",
      "Epoch 642/1000 : Loss = 38270.23631636756\n",
      "Epoch 643/1000 : Loss = 38259.005592099726\n",
      "Epoch 644/1000 : Loss = 38247.79484255709\n",
      "Epoch 645/1000 : Loss = 38236.60434724654\n",
      "Epoch 646/1000 : Loss = 38225.434381605286\n",
      "Epoch 647/1000 : Loss = 38214.28521701003\n",
      "Epoch 648/1000 : Loss = 38203.157120787226\n",
      "Epoch 649/1000 : Loss = 38192.05035622507\n",
      "Epoch 650/1000 : Loss = 38180.96518258589\n",
      "Epoch 651/1000 : Loss = 38169.901855120355\n",
      "Epoch 652/1000 : Loss = 38158.86062508262\n",
      "Epoch 653/1000 : Loss = 38147.84173974629\n",
      "Epoch 654/1000 : Loss = 38136.84544242192\n",
      "Epoch 655/1000 : Loss = 38125.871972475274\n",
      "Epoch 656/1000 : Loss = 38114.9215653467\n",
      "Epoch 657/1000 : Loss = 38103.99445257155\n",
      "Epoch 658/1000 : Loss = 38093.09086180137\n",
      "Epoch 659/1000 : Loss = 38082.21101682637\n",
      "Epoch 660/1000 : Loss = 38071.35513759851\n",
      "Epoch 661/1000 : Loss = 38060.52344025557\n",
      "Epoch 662/1000 : Loss = 38049.71613714632\n",
      "Epoch 663/1000 : Loss = 38038.93343685596\n",
      "Epoch 664/1000 : Loss = 38028.17554423306\n",
      "Epoch 665/1000 : Loss = 38017.44266041674\n",
      "Epoch 666/1000 : Loss = 38006.73498286497\n",
      "Epoch 667/1000 : Loss = 37996.05270538326\n",
      "Epoch 668/1000 : Loss = 37985.39601815444\n",
      "Epoch 669/1000 : Loss = 37974.76510776869\n",
      "Epoch 670/1000 : Loss = 37964.1601572546\n",
      "Epoch 671/1000 : Loss = 37953.58134611065\n",
      "Epoch 672/1000 : Loss = 37943.0288503373\n",
      "Epoch 673/1000 : Loss = 37932.502842469665\n",
      "Epoch 674/1000 : Loss = 37922.00349161064\n",
      "Epoch 675/1000 : Loss = 37911.530963464975\n",
      "Epoch 676/1000 : Loss = 37901.08542037301\n",
      "Epoch 677/1000 : Loss = 37890.667021345864\n",
      "Epoch 678/1000 : Loss = 37880.27592210016\n",
      "Epoch 679/1000 : Loss = 37869.91227509389\n",
      "Epoch 680/1000 : Loss = 37859.57622956209\n",
      "Epoch 681/1000 : Loss = 37849.26793155334\n",
      "Epoch 682/1000 : Loss = 37838.98752396633\n",
      "Epoch 683/1000 : Loss = 37828.73514658687\n",
      "Epoch 684/1000 : Loss = 37818.510936125065\n",
      "Epoch 685/1000 : Loss = 37808.31502625298\n",
      "Epoch 686/1000 : Loss = 37798.14754764221\n",
      "Epoch 687/1000 : Loss = 37788.00862800214\n",
      "Epoch 688/1000 : Loss = 37777.89839211799\n",
      "Epoch 689/1000 : Loss = 37767.8169618893\n",
      "Epoch 690/1000 : Loss = 37757.764456368364\n",
      "Epoch 691/1000 : Loss = 37747.74099179899\n",
      "Epoch 692/1000 : Loss = 37737.74668165543\n",
      "Epoch 693/1000 : Loss = 37727.78163668114\n",
      "Epoch 694/1000 : Loss = 37717.845964927736\n",
      "Epoch 695/1000 : Loss = 37707.939771794176\n",
      "Epoch 696/1000 : Loss = 37698.06316006588\n",
      "Epoch 697/1000 : Loss = 37688.21622995371\n",
      "Epoch 698/1000 : Loss = 37678.3990791331\n",
      "Epoch 699/1000 : Loss = 37668.61180278322\n",
      "Epoch 700/1000 : Loss = 37658.85449362612\n",
      "Epoch 701/1000 : Loss = 37649.127241965616\n",
      "Epoch 702/1000 : Loss = 37639.43013572619\n",
      "Epoch 703/1000 : Loss = 37629.763260492145\n",
      "Epoch 704/1000 : Loss = 37620.12669954614\n",
      "Epoch 705/1000 : Loss = 37610.52053390793\n",
      "Epoch 706/1000 : Loss = 37600.94484237293\n",
      "Epoch 707/1000 : Loss = 37591.39970155059\n",
      "Epoch 708/1000 : Loss = 37581.8851859026\n",
      "Epoch 709/1000 : Loss = 37572.40136778103\n",
      "Epoch 710/1000 : Loss = 37562.948317466245\n",
      "Epoch 711/1000 : Loss = 37553.52610320444\n",
      "Epoch 712/1000 : Loss = 37544.13479124537\n",
      "Epoch 713/1000 : Loss = 37534.774445879375\n",
      "Epoch 714/1000 : Loss = 37525.44512947452\n",
      "Epoch 715/1000 : Loss = 37516.146902513494\n",
      "Epoch 716/1000 : Loss = 37506.87982362987\n",
      "Epoch 717/1000 : Loss = 37497.64394964471\n",
      "Epoch 718/1000 : Loss = 37488.43933560236\n",
      "Epoch 719/1000 : Loss = 37479.266034806176\n",
      "Epoch 720/1000 : Loss = 37470.12409885403\n",
      "Epoch 721/1000 : Loss = 37461.01357767341\n",
      "Epoch 722/1000 : Loss = 37451.93451955615\n",
      "Epoch 723/1000 : Loss = 37442.886971193075\n",
      "Epoch 724/1000 : Loss = 37433.870977708044\n",
      "Epoch 725/1000 : Loss = 37424.88658269182\n",
      "Epoch 726/1000 : Loss = 37415.933828235706\n",
      "Epoch 727/1000 : Loss = 37407.01275496444\n",
      "Epoch 728/1000 : Loss = 37398.12340206929\n",
      "Epoch 729/1000 : Loss = 37389.26580734016\n",
      "Epoch 730/1000 : Loss = 37380.440007198034\n",
      "Epoch 731/1000 : Loss = 37371.64603672645\n",
      "Epoch 732/1000 : Loss = 37362.88392970292\n",
      "Epoch 733/1000 : Loss = 37354.153718629896\n",
      "Epoch 734/1000 : Loss = 37345.45543476527\n",
      "Epoch 735/1000 : Loss = 37336.7891081527\n",
      "Epoch 736/1000 : Loss = 37328.15476765137\n",
      "Epoch 737/1000 : Loss = 37319.552440965286\n",
      "Epoch 738/1000 : Loss = 37310.9821546725\n",
      "Epoch 739/1000 : Loss = 37302.44393425361\n",
      "Epoch 740/1000 : Loss = 37293.937804119945\n",
      "Epoch 741/1000 : Loss = 37285.463787641485\n",
      "Epoch 742/1000 : Loss = 37277.02190717417\n",
      "Epoch 743/1000 : Loss = 37268.612184087084\n",
      "Epoch 744/1000 : Loss = 37260.234638788854\n",
      "Epoch 745/1000 : Loss = 37251.88929075394\n",
      "Epoch 746/1000 : Loss = 37243.57615854848\n",
      "Epoch 747/1000 : Loss = 37235.29525985547\n",
      "Epoch 748/1000 : Loss = 37227.04661150008\n",
      "Epoch 749/1000 : Loss = 37218.830229473824\n",
      "Epoch 750/1000 : Loss = 37210.646128958964\n",
      "Epoch 751/1000 : Loss = 37202.49432435221\n",
      "Epoch 752/1000 : Loss = 37194.374829287946\n",
      "Epoch 753/1000 : Loss = 37186.287656661385\n",
      "Epoch 754/1000 : Loss = 37178.23281865084\n",
      "Epoch 755/1000 : Loss = 37170.21032673992\n",
      "Epoch 756/1000 : Loss = 37162.220191739376\n",
      "Epoch 757/1000 : Loss = 37154.26242380824\n",
      "Epoch 758/1000 : Loss = 37146.33703247482\n",
      "Epoch 759/1000 : Loss = 37138.444026657286\n",
      "Epoch 760/1000 : Loss = 37130.5834146835\n",
      "Epoch 761/1000 : Loss = 37122.75520431117\n",
      "Epoch 762/1000 : Loss = 37114.95940274685\n",
      "Epoch 763/1000 : Loss = 37107.19601666499\n",
      "Epoch 764/1000 : Loss = 37099.4650522265\n",
      "Epoch 765/1000 : Loss = 37091.76651509691\n",
      "Epoch 766/1000 : Loss = 37084.10041046419\n",
      "Epoch 767/1000 : Loss = 37076.466743056095\n",
      "Epoch 768/1000 : Loss = 37068.86551715737\n",
      "Epoch 769/1000 : Loss = 37061.29673662609\n",
      "Epoch 770/1000 : Loss = 37053.76040491042\n",
      "Epoch 771/1000 : Loss = 37046.256525064084\n",
      "Epoch 772/1000 : Loss = 37038.7850997623\n",
      "Epoch 773/1000 : Loss = 37031.34613131681\n",
      "Epoch 774/1000 : Loss = 37023.93962169075\n",
      "Epoch 775/1000 : Loss = 37016.56557251328\n",
      "Epoch 776/1000 : Loss = 37009.22398509363\n",
      "Epoch 777/1000 : Loss = 37001.914860435\n",
      "Epoch 778/1000 : Loss = 36994.63819924797\n",
      "Epoch 779/1000 : Loss = 36987.39400196377\n",
      "Epoch 780/1000 : Loss = 36980.182268747\n",
      "Epoch 781/1000 : Loss = 36973.00299950818\n",
      "Epoch 782/1000 : Loss = 36965.856193915875\n",
      "Epoch 783/1000 : Loss = 36958.741851408675\n",
      "Epoch 784/1000 : Loss = 36951.659971206594\n",
      "Epoch 785/1000 : Loss = 36944.61055232243\n",
      "Epoch 786/1000 : Loss = 36937.59359357252\n",
      "Epoch 787/1000 : Loss = 36930.609093587664\n",
      "Epoch 788/1000 : Loss = 36923.65705082322\n",
      "Epoch 789/1000 : Loss = 36916.73746356916\n",
      "Epoch 790/1000 : Loss = 36909.85032996001\n",
      "Epoch 791/1000 : Loss = 36902.995647984215\n",
      "Epoch 792/1000 : Loss = 36896.1734154932\n",
      "Epoch 793/1000 : Loss = 36889.38363021061\n",
      "Epoch 794/1000 : Loss = 36882.62628974073\n",
      "Epoch 795/1000 : Loss = 36875.90139157694\n",
      "Epoch 796/1000 : Loss = 36869.20893310989\n",
      "Epoch 797/1000 : Loss = 36862.54891163536\n",
      "Epoch 798/1000 : Loss = 36855.92132436183\n",
      "Epoch 799/1000 : Loss = 36849.326168418\n",
      "Epoch 800/1000 : Loss = 36842.76344085978\n",
      "Epoch 801/1000 : Loss = 36836.23313867731\n",
      "Epoch 802/1000 : Loss = 36829.73525880151\n",
      "Epoch 803/1000 : Loss = 36823.269798110734\n",
      "Epoch 804/1000 : Loss = 36816.836753436786\n",
      "Epoch 805/1000 : Loss = 36810.43612157099\n",
      "Epoch 806/1000 : Loss = 36804.06789926998\n",
      "Epoch 807/1000 : Loss = 36797.73208326132\n",
      "Epoch 808/1000 : Loss = 36791.42867024876\n",
      "Epoch 809/1000 : Loss = 36785.157656917574\n",
      "Epoch 810/1000 : Loss = 36778.91903993933\n",
      "Epoch 811/1000 : Loss = 36772.71281597678\n",
      "Epoch 812/1000 : Loss = 36766.53898168848\n",
      "Epoch 813/1000 : Loss = 36760.39753373308\n",
      "Epoch 814/1000 : Loss = 36754.28846877364\n",
      "Epoch 815/1000 : Loss = 36748.2117834816\n",
      "Epoch 816/1000 : Loss = 36742.1674745407\n",
      "Epoch 817/1000 : Loss = 36736.15553865069\n",
      "Epoch 818/1000 : Loss = 36730.17597253078\n",
      "Epoch 819/1000 : Loss = 36724.22877292308\n",
      "Epoch 820/1000 : Loss = 36718.31393659581\n",
      "Epoch 821/1000 : Loss = 36712.43146034639\n",
      "Epoch 822/1000 : Loss = 36706.58134100426\n",
      "Epoch 823/1000 : Loss = 36700.763575433804\n",
      "Epoch 824/1000 : Loss = 36694.978160536695\n",
      "Epoch 825/1000 : Loss = 36689.225093254754\n",
      "Epoch 826/1000 : Loss = 36683.50437057198\n",
      "Epoch 827/1000 : Loss = 36677.81598951697\n",
      "Epoch 828/1000 : Loss = 36672.1599471649\n",
      "Epoch 829/1000 : Loss = 36666.53624063947\n",
      "Epoch 830/1000 : Loss = 36660.94486711495\n",
      "Epoch 831/1000 : Loss = 36655.38582381765\n",
      "Epoch 832/1000 : Loss = 36649.85910802769\n",
      "Epoch 833/1000 : Loss = 36644.36471708041\n",
      "Epoch 834/1000 : Loss = 36638.90264836791\n",
      "Epoch 835/1000 : Loss = 36633.47289934011\n",
      "Epoch 836/1000 : Loss = 36628.075467506154\n",
      "Epoch 837/1000 : Loss = 36622.71035043543\n",
      "Epoch 838/1000 : Loss = 36617.37754575847\n",
      "Epoch 839/1000 : Loss = 36612.07705116798\n",
      "Epoch 840/1000 : Loss = 36606.808864419545\n",
      "Epoch 841/1000 : Loss = 36601.57298333247\n",
      "Epoch 842/1000 : Loss = 36596.36940579031\n",
      "Epoch 843/1000 : Loss = 36591.19812974145\n",
      "Epoch 844/1000 : Loss = 36586.05915319963\n",
      "Epoch 845/1000 : Loss = 36580.95247424432\n",
      "Epoch 846/1000 : Loss = 36575.878091020946\n",
      "Epoch 847/1000 : Loss = 36570.83600174149\n",
      "Epoch 848/1000 : Loss = 36565.826204684134\n",
      "Epoch 849/1000 : Loss = 36560.8486981939\n",
      "Epoch 850/1000 : Loss = 36555.90348068243\n",
      "Epoch 851/1000 : Loss = 36550.990550628034\n",
      "Epoch 852/1000 : Loss = 36546.109906575686\n",
      "Epoch 853/1000 : Loss = 36541.261547136804\n",
      "Epoch 854/1000 : Loss = 36536.445470989194\n",
      "Epoch 855/1000 : Loss = 36531.661676876734\n",
      "Epoch 856/1000 : Loss = 36526.91016360918\n",
      "Epoch 857/1000 : Loss = 36522.19093006179\n",
      "Epoch 858/1000 : Loss = 36517.5039751749\n",
      "Epoch 859/1000 : Loss = 36512.84929795357\n",
      "Epoch 860/1000 : Loss = 36508.2268974673\n",
      "Epoch 861/1000 : Loss = 36503.63677284914\n",
      "Epoch 862/1000 : Loss = 36499.078923295514\n",
      "Epoch 863/1000 : Loss = 36494.55334806547\n",
      "Epoch 864/1000 : Loss = 36490.060046480125\n",
      "Epoch 865/1000 : Loss = 36485.599017922\n",
      "Epoch 866/1000 : Loss = 36481.17026183433\n",
      "Epoch 867/1000 : Loss = 36476.77377772048\n",
      "Epoch 868/1000 : Loss = 36472.40956514309\n",
      "Epoch 869/1000 : Loss = 36468.07762372325\n",
      "Epoch 870/1000 : Loss = 36463.777953139965\n",
      "Epoch 871/1000 : Loss = 36459.510553129105\n",
      "Epoch 872/1000 : Loss = 36455.27542348268\n",
      "Epoch 873/1000 : Loss = 36451.07256404804\n",
      "Epoch 874/1000 : Loss = 36446.9019747268\n",
      "Epoch 875/1000 : Loss = 36442.76365547425\n",
      "Epoch 876/1000 : Loss = 36438.6576062981\n",
      "Epoch 877/1000 : Loss = 36434.58382725789\n",
      "Epoch 878/1000 : Loss = 36430.54231846387\n",
      "Epoch 879/1000 : Loss = 36426.53308007597\n",
      "Epoch 880/1000 : Loss = 36422.556112303006\n",
      "Epoch 881/1000 : Loss = 36418.611415401676\n",
      "Epoch 882/1000 : Loss = 36414.69898967536\n",
      "Epoch 883/1000 : Loss = 36410.81883547351\n",
      "Epoch 884/1000 : Loss = 36406.97095319018\n",
      "Epoch 885/1000 : Loss = 36403.155343263476\n",
      "Epoch 886/1000 : Loss = 36399.37200617408\n",
      "Epoch 887/1000 : Loss = 36395.62094244462\n",
      "Epoch 888/1000 : Loss = 36391.902152638344\n",
      "Epoch 889/1000 : Loss = 36388.21563735828\n",
      "Epoch 890/1000 : Loss = 36384.56139724604\n",
      "Epoch 891/1000 : Loss = 36380.93943298096\n",
      "Epoch 892/1000 : Loss = 36377.34974527874\n",
      "Epoch 893/1000 : Loss = 36373.79233489087\n",
      "Epoch 894/1000 : Loss = 36370.267202603085\n",
      "Epoch 895/1000 : Loss = 36366.774349234656\n",
      "Epoch 896/1000 : Loss = 36363.31377563722\n",
      "Epoch 897/1000 : Loss = 36359.88548269372\n",
      "Epoch 898/1000 : Loss = 36356.48947131736\n",
      "Epoch 899/1000 : Loss = 36353.12574245072\n",
      "Epoch 900/1000 : Loss = 36349.79429706449\n",
      "Epoch 901/1000 : Loss = 36346.495136156445\n",
      "Epoch 902/1000 : Loss = 36343.22826075072\n",
      "Epoch 903/1000 : Loss = 36339.99367189642\n",
      "Epoch 904/1000 : Loss = 36336.79137066683\n",
      "Epoch 905/1000 : Loss = 36333.62135815823\n",
      "Epoch 906/1000 : Loss = 36330.4836354891\n",
      "Epoch 907/1000 : Loss = 36327.37820379888\n",
      "Epoch 908/1000 : Loss = 36324.30506424711\n",
      "Epoch 909/1000 : Loss = 36321.2642180125\n",
      "Epoch 910/1000 : Loss = 36318.255666291756\n",
      "Epoch 911/1000 : Loss = 36315.279410298834\n",
      "Epoch 912/1000 : Loss = 36312.33545126378\n",
      "Epoch 913/1000 : Loss = 36309.42379043186\n",
      "Epoch 914/1000 : Loss = 36306.54442906262\n",
      "Epoch 915/1000 : Loss = 36303.69736842895\n",
      "Epoch 916/1000 : Loss = 36300.8826098161\n",
      "Epoch 917/1000 : Loss = 36298.10015452076\n",
      "Epoch 918/1000 : Loss = 36295.350003850275\n",
      "Epoch 919/1000 : Loss = 36292.632159121575\n",
      "Epoch 920/1000 : Loss = 36289.94662166038\n",
      "Epoch 921/1000 : Loss = 36287.29339280023\n",
      "Epoch 922/1000 : Loss = 36284.67247388183\n",
      "Epoch 923/1000 : Loss = 36282.08386625191\n",
      "Epoch 924/1000 : Loss = 36279.52757126252\n",
      "Epoch 925/1000 : Loss = 36277.00359027022\n",
      "Epoch 926/1000 : Loss = 36274.511924635124\n",
      "Epoch 927/1000 : Loss = 36272.052575720256\n",
      "Epoch 928/1000 : Loss = 36269.62554489058\n",
      "Epoch 929/1000 : Loss = 36267.23083351222\n",
      "Epoch 930/1000 : Loss = 36264.868442951934\n",
      "Epoch 931/1000 : Loss = 36262.538374575925\n",
      "Epoch 932/1000 : Loss = 36260.24062974938\n",
      "Epoch 933/1000 : Loss = 36257.975209835626\n",
      "Epoch 934/1000 : Loss = 36255.74211619536\n",
      "Epoch 935/1000 : Loss = 36253.54135018603\n",
      "Epoch 936/1000 : Loss = 36251.37291316099\n",
      "Epoch 937/1000 : Loss = 36249.23680646875\n",
      "Epoch 938/1000 : Loss = 36247.13303145266\n",
      "Epoch 939/1000 : Loss = 36245.06158944969\n",
      "Epoch 940/1000 : Loss = 36243.02248179016\n",
      "Epoch 941/1000 : Loss = 36241.01570979688\n",
      "Epoch 942/1000 : Loss = 36239.041274784635\n",
      "Epoch 943/1000 : Loss = 36237.0991780594\n",
      "Epoch 944/1000 : Loss = 36235.18942091799\n",
      "Epoch 945/1000 : Loss = 36233.31200464697\n",
      "Epoch 946/1000 : Loss = 36231.46693052261\n",
      "Epoch 947/1000 : Loss = 36229.65419980986\n",
      "Epoch 948/1000 : Loss = 36227.87381376208\n",
      "Epoch 949/1000 : Loss = 36226.12577362027\n",
      "Epoch 950/1000 : Loss = 36224.41008061259\n",
      "Epoch 951/1000 : Loss = 36222.72673595378\n",
      "Epoch 952/1000 : Loss = 36221.075740844775\n",
      "Epoch 953/1000 : Loss = 36219.457096472\n",
      "Epoch 954/1000 : Loss = 36217.87080400686\n",
      "Epoch 955/1000 : Loss = 36216.31686460548\n",
      "Epoch 956/1000 : Loss = 36214.79527940794\n",
      "Epoch 957/1000 : Loss = 36213.306049537954\n",
      "Epoch 958/1000 : Loss = 36211.84917610237\n",
      "Epoch 959/1000 : Loss = 36210.42466019061\n",
      "Epoch 960/1000 : Loss = 36209.03250287453\n",
      "Epoch 961/1000 : Loss = 36207.67270520759\n",
      "Epoch 962/1000 : Loss = 36206.34526822477\n",
      "Epoch 963/1000 : Loss = 36205.05019294192\n",
      "Epoch 964/1000 : Loss = 36203.78748035552\n",
      "Epoch 965/1000 : Loss = 36202.55713144216\n",
      "Epoch 966/1000 : Loss = 36201.359147158335\n",
      "Epoch 967/1000 : Loss = 36200.193528439835\n",
      "Epoch 968/1000 : Loss = 36199.06027620167\n",
      "Epoch 969/1000 : Loss = 36197.95939133749\n",
      "Epoch 970/1000 : Loss = 36196.890874719254\n",
      "Epoch 971/1000 : Loss = 36195.854727197104\n",
      "Epoch 972/1000 : Loss = 36194.85094959893\n",
      "Epoch 973/1000 : Loss = 36193.87954272992\n",
      "Epoch 974/1000 : Loss = 36192.940507372536\n",
      "Epoch 975/1000 : Loss = 36192.03384428596\n",
      "Epoch 976/1000 : Loss = 36191.159554206024\n",
      "Epoch 977/1000 : Loss = 36190.31763784483\n",
      "Epoch 978/1000 : Loss = 36189.508095890524\n",
      "Epoch 979/1000 : Loss = 36188.73092900705\n",
      "Epoch 980/1000 : Loss = 36187.98613783386\n",
      "Epoch 981/1000 : Loss = 36187.273722985796\n",
      "Epoch 982/1000 : Loss = 36186.593685052685\n",
      "Epoch 983/1000 : Loss = 36185.94602459936\n",
      "Epoch 984/1000 : Loss = 36185.330742165286\n",
      "Epoch 985/1000 : Loss = 36184.747838264324\n",
      "Epoch 986/1000 : Loss = 36184.19731338483\n",
      "Epoch 987/1000 : Loss = 36183.67916798916\n",
      "Epoch 988/1000 : Loss = 36183.19340251366\n",
      "Epoch 989/1000 : Loss = 36182.740017368495\n",
      "Epoch 990/1000 : Loss = 36182.31901293748\n",
      "Epoch 991/1000 : Loss = 36181.930389578\n",
      "Epoch 992/1000 : Loss = 36181.574147620726\n",
      "Epoch 993/1000 : Loss = 36181.25028736974\n",
      "Epoch 994/1000 : Loss = 36180.958809102194\n",
      "Epoch 995/1000 : Loss = 36180.699713068345\n",
      "Epoch 996/1000 : Loss = 36180.472999491336\n",
      "Epoch 997/1000 : Loss = 36180.27866856732\n",
      "Epoch 998/1000 : Loss = 36180.11672046515\n",
      "Epoch 999/1000 : Loss = 36179.987155326446\n",
      "Epoch 1000/1000 : Loss = 36179.88997326554\n"
     ]
    }
   ],
   "source": [
    "model = LatentFactorDecomposition(train_matrix, f = 5)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, learning is occuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631422584701616"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lfd(model, expert_matrix, test_users_start, test_tags_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to complete all deliverables for 4th part and 5th part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
