{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Dataset\n",
    "- Following Script has been used to extract dataset into `csv` folder\n",
    "- Make sure to install `go`, `git`, `gdown` and `7z`.\n",
    "- `softwareengineering.stackexchange.com.7z` should also be present in same directory.\n",
    "- The dataset is now directly read from `csv` folder. Kindly run the script before executing. \n",
    "\n",
    "```bash\n",
    "gdown --id 19UhOq9Z5IVqM926cC3hvxcl726CTv-kT\n",
    "mkdir xml csv\n",
    "7z e softwareengineering.stackexchange.com.7z -oxml\n",
    "git clone https://github.com/SkobelevIgor/stackexchange-xml-converter\n",
    "cd stackexchange-xml-converter/\n",
    "go build\n",
    "./stackexchange-xml-converter -result-format=csv -source-path=../xml -store-to-dir=../csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer = pd.read_csv('csv/Posts.csv') # Posts.csv should be there in csv/ extracted using above code.\n",
    "question_answer.set_index('Id', inplace=True)\n",
    "answers = question_answer[question_answer['PostTypeId'] == 2]\n",
    "questions = question_answer[question_answer['PostTypeId'] == 1]\n",
    "# this observation has come from the fact that, PostTypeId == 2, have a parent Id, while those with 1 have answerCount field non empty, rest all ids are wikis etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only counting the unique questions answered by a user, so we are using set here.\n",
    "answerer = answers.groupby('OwnerUserId')['ParentId'].apply(set).to_dict()\n",
    "\n",
    "# Computing question tags\n",
    "question_tags = questions['Tags'].apply(lambda x: list(filter(lambda x: x != '', x.split('|'))))\n",
    "exploded_tags = question_tags.explode()\n",
    "tags = exploded_tags.value_counts()\n",
    "count_tag = tags.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6822"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['OwnerUserId'].isna().sum()\n",
    "# 6822 answers are such that their owner is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to count only unique answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_answers_per_user = {}\n",
    "for key, val in answerer.items():\n",
    "  count_answers_per_user[key] = len(val)\n",
    "\n",
    "count_answers_per_user = dict(sorted(count_answers_per_user.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting TagNames to TagIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.read_csv('csv/Tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Id', 'ExcerptPostId', 'WikiPostId', 'TagName', 'Count'], dtype='object'),\n",
       " ['ExcerptPostId', 'WikiPostId', 'Count'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags_df.set_index('Id', inplace=True)\n",
    "tag_cols = tags_df.columns\n",
    "req_tag_cols = ['Id', 'TagName']\n",
    "drop_cols = list(filter(lambda x: x not in req_tag_cols, tag_cols))\n",
    "tag_cols, drop_cols # we will drop drop cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tags_df = tags_df.drop(drop_cols, axis=1)\n",
    "new_tags_df.set_index('TagName', inplace=True)\n",
    "tag_dict = new_tags_df.to_dict()['Id']\n",
    "# some tags not there in the tags.csv file, don't know why :(\n",
    "count_tag = {tag_dict[k]: v for k, v in count_tag.items() if k in tag_dict} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>AnsweredQuestionCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9113.0</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177980.0</td>\n",
       "      <td>2318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1204.0</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123788.0</td>\n",
       "      <td>1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131624.0</td>\n",
       "      <td>1602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserId  AnsweredQuestionCount\n",
       "0    9113.0                   2838\n",
       "1  177980.0                   2318\n",
       "2    1204.0                   2042\n",
       "3  123788.0                   1672\n",
       "4  131624.0                   1602"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerer_table = pd.DataFrame(count_answers_per_user.items(), columns=['UserId', 'AnsweredQuestionCount'])\n",
    "answerer_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TagId</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>609</td>\n",
       "      <td>5162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249</td>\n",
       "      <td>4931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>4928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>391</td>\n",
       "      <td>4449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>790</td>\n",
       "      <td>3510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TagId  Count\n",
       "0    609   5162\n",
       "1    249   4931\n",
       "2     76   4928\n",
       "3    391   4449\n",
       "4    790   3510"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_table = pd.DataFrame(count_tag.items(), columns=['TagId', 'Count'])\n",
    "tags_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TagId</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>609</td>\n",
       "      <td>5162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249</td>\n",
       "      <td>4931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>4928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TagId  Count\n",
       "0    609   5162\n",
       "1    249   4931\n",
       "2     76   4928"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting top 3 most used tags\n",
    "sorted_tags_table = tags_table.sort_values(by='Count', ascending=False)\n",
    "sorted_tags_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>AnsweredQuestionCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9113.0</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177980.0</td>\n",
       "      <td>2318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1204.0</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserId  AnsweredQuestionCount\n",
       "0    9113.0                   2838\n",
       "1  177980.0                   2318\n",
       "2    1204.0                   2042"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP 3 answerers\n",
    "sorted_answerer_table = answerer_table.sort_values(by='AnsweredQuestionCount', ascending=False)\n",
    "sorted_answerer_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20\n",
    "answerer_table = answerer_table[answerer_table['AnsweredQuestionCount'] >= threshold]\n",
    "tags_table = tags_table[tags_table['Count'] >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_tag_count wlll be a dictionary of dictionaries, with UserId as key and a dictionary of tags as value.\n",
    "user_tag_count = {}\n",
    "tags_table = tags_table.sort_values(['TagId'])\n",
    "answerer_table = answerer_table.sort_values(['UserId'])\n",
    "users = answerer_table['UserId'].values\n",
    "tags = tags_table['TagId'].values\n",
    "q_tag_map = question_tags.to_dict()\n",
    "\n",
    "\n",
    "for user in users:\n",
    "  tag_count = {tag : 0 for tag in tags}\n",
    "  for question in answerer[user]:\n",
    "    for tag in q_tag_map[question]:\n",
    "      try:\n",
    "        tag_count[tag_dict[tag]] += 1\n",
    "      except KeyError:\n",
    "        pass\n",
    "  user_tag_count[user] = tag_count\n",
    "del q_tag_map\n",
    "del users\n",
    "del tags # to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = pd.DataFrame.from_dict(user_tag_count, orient='index')\n",
    "del user_tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>4639</th>\n",
       "      <th>4646</th>\n",
       "      <th>4661</th>\n",
       "      <th>4682</th>\n",
       "      <th>4683</th>\n",
       "      <th>4687</th>\n",
       "      <th>4690</th>\n",
       "      <th>4704</th>\n",
       "      <th>4720</th>\n",
       "      <th>4750</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 973 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1     3     4     7     8     9     11    12    13    14    ...  4639  \\\n",
       "4.0     13     0     6     6    61    55     8     3     0     0  ...     0   \n",
       "6.0      0     0     8     0     6     4     1     2     0     0  ...     0   \n",
       "11.0     1     0     1     0     0     1     0     1     0     0  ...     0   \n",
       "14.0     0     0     1     0     1     1     0     1     0     0  ...     0   \n",
       "15.0     1     0     2     1     4     4     1     1     0     0  ...     0   \n",
       "\n",
       "      4646  4661  4682  4683  4687  4690  4704  4720  4750  \n",
       "4.0      0     0     2     1     1     0     0     1     0  \n",
       "6.0      0     0     0     0     0     0     0     0     0  \n",
       "11.0     0     0     0     0     0     0     0     0     0  \n",
       "14.0     0     0     0     0     0     0     0     0     0  \n",
       "15.0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 973 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160, 973)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert.shape # 1160 users, 973 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = expert.map(lambda x: float(x//3) if x < 15 else 5.0)\n",
    "# expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_matrix = expert.to_numpy()\n",
    "expert_shape = expert_matrix.shape\n",
    "# expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(986, 827)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_start = (int(0.85 * expert_shape[0] ), int(0.85 * expert_shape[1]))\n",
    "test_users, test_tags = test_start\n",
    "test_start\n",
    "# As the number 0.85 * expert_shape[0] is not an integer, we can take its floor.\n",
    "# Some may choose to take the ceiling, hence a difference of 1 is possible in the test_users, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 146)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Not using a copy for test_matrix will lead to changes in expert_matrix as well, as numpy arrays are mutable\n",
    "# This is a common mistake, but could lead to better answers ;) \n",
    "test_matrix = np.copy(expert_matrix[test_users:, test_tags:])\n",
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = np.copy(expert_matrix)\n",
    "train_matrix[test_users:, test_tags:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41180.0, 1162.0, 1403.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Report the following about your utility matrix:\n",
    "Summation value of the utility matrix\n",
    "Highest row sum of the utility matrix\n",
    "Highest column sum of the utility matrix\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sum_utility = np.nansum(expert_matrix)\n",
    "highest_row_sum = np.nansum(expert_matrix, axis = 1).max()\n",
    "highest_col_sum = np.nansum(expert_matrix, axis = 0).max()\n",
    "sum_utility, highest_row_sum, highest_col_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40538.0, (174, 146), 642.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Report the following for your train and test data\n",
    "Summation value of the train matrix\n",
    "Dimension of the test matrix\n",
    "Summation value of test matrix\n",
    "\"\"\"\n",
    "\n",
    "sum_train = np.nansum(train_matrix)\n",
    "dim_test = test_matrix.shape\n",
    "sum_test = np.nansum(test_matrix)\n",
    "sum_train, dim_test, sum_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `CollaborativeFilter` is sort of an abstract class which will provide implementable methods for the actual collaborative filter classes. The actual collaborative filter classes will inherit from this class and implement the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users_start, test_tags_start = test_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "class CollaborativeFilter():\n",
    "  def __init__(self, utility_matrix, function='weighted'):\n",
    "    self.utility_matrix = utility_matrix\n",
    "    self.epsilon = 1e-9 # a small value to avoid division by zero\n",
    "    \n",
    "    self.final_rating_function = None\n",
    "    if function == 'weighted':\n",
    "      self.final_rating_function = self.weighted_average\n",
    "    elif function=='regular_average':\n",
    "      self.final_rating_function = self.average\n",
    "    else:\n",
    "      raise Exception(\"Function not allowed\")\n",
    "    \n",
    "  \n",
    "  def weighted_average(self, vector: np.ndarray, scores: np.ndarray):\n",
    "    return np.dot(vector, scores)/(np.sum(scores) + self.epsilon)\n",
    "  \n",
    "  def average(self, vector: np.ndarray, scores: np.ndarray):\n",
    "    return np.mean(vector)\n",
    "    \n",
    "\n",
    "  @abstractmethod\n",
    "  def predict(self):\n",
    "    pass\n",
    "  \n",
    "  \n",
    "  @abstractmethod\n",
    "  def compute_similarities(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`UserBasedCollaborativeFilter` class implements the collaborative filtering algorithm based on users. It will inherit from `CollaborativeFilter` and implement the methods.It computes the similarity between users based on the ratings they have given to the items. It then predicts the rating of a user for an item by taking `k` most similar users who have rated the item and computing the appropriate average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedCollaborativeFilter(CollaborativeFilter):\n",
    "    def __init__(self,expert_matrix ,k , function):\n",
    "      \"\"\"\n",
    "      utility_matrix : 2D numpy array\n",
    "      k : int\n",
    "      \n",
    "      k is the number of similar users to consider for prediction\n",
    "      utility_matrix is the matrix of user ratings, nan filled with 0\n",
    "      \"\"\"\n",
    "      super().__init__(expert_matrix, function)\n",
    "      self.k = k\n",
    "      self.sim = dict()\n",
    "        \n",
    "    \n",
    "    def compute_similarities(self, user_vector : np.ndarray, index: int):\n",
    "      \"\"\"\n",
    "      user_vector : 1D numpy array\n",
    "      \"\"\"\n",
    "      if index in self.sim:\n",
    "        return self.sim[index]\n",
    "      \n",
    "      utility_rating_means = np.nanmean(self.utility_matrix, axis=0)\n",
    "      user_rating_mean = np.nanmean(user_vector)\n",
    "\n",
    "      utility_matrix_centered = self.utility_matrix - utility_rating_means\n",
    "      \n",
    "      user_vector_centered = user_vector - user_rating_mean\n",
    "      \n",
    "      user_vector_norm = np.sqrt(np.nansum(user_vector_centered**2))\n",
    "      utility_matrix_norm = np.sqrt(np.nansum(utility_matrix_centered**2, axis=1))\n",
    "      \n",
    "      dot_product = np.nansum(utility_matrix_centered *user_vector_centered, axis = 1)\n",
    "\n",
    "      similarities = dot_product/(user_vector_norm * utility_matrix_norm + self.epsilon)\n",
    "      self.sim[index] = similarities\n",
    "      return similarities\n",
    "    \n",
    "    def find_k_nearest_users(self, user_vector : np.ndarray, i: int, user_index: int):\n",
    "      \"\"\"\n",
    "      user_vector : 1D numpy array\n",
    "      \n",
    "      returns the indices of the k most similar users to user_vector\n",
    "      \"\"\"\n",
    "      \n",
    "      similarities = self.compute_similarities(user_vector, user_index)\n",
    "      users_to_consider = np.where(~np.isnan(self.utility_matrix[:, i]))[0]\n",
    "      k = min(self.k, len(users_to_consider))\n",
    "      sorted_indices = np.argsort(similarities[users_to_consider])[-k:]\n",
    "      users_to_consider = users_to_consider[sorted_indices]\n",
    "      similarities = similarities[users_to_consider]\n",
    "      return similarities, users_to_consider\n",
    "    \n",
    "    \n",
    "    def predict(self, user_vector : np.ndarray, i: int, user_index: int):\n",
    "      \"\"\"\n",
    "      user_vector : 1D numpy array\n",
    "      i : int\n",
    "      \n",
    "      i is the index of the item to predict the rating for\n",
    "      \"\"\"\n",
    "      \n",
    "      # find the k most similar users who rated this item \n",
    "      similarities, users_to_consider = self.find_k_nearest_users(user_vector, i, user_index)\n",
    "      # similarity_scores = similarities[users_to_consider]\n",
    "      \n",
    "      # predict the rating for the item\n",
    "      prediction = self.final_rating_function(self.utility_matrix[users_to_consider, i], similarities)\n",
    "      return prediction\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tests on the model\n",
    "def test_user_collaborator(train_matrix: np.ndarray, expert_matrix: np.ndarray, user_start: int, item_start: int, model: UserBasedCollaborativeFilter) -> float:\n",
    "    rmse_loss = 0\n",
    "    count = 0\n",
    "    for i in range(user_start, expert_matrix.shape[0]):\n",
    "        for j in range(item_start, expert_matrix.shape[1]):\n",
    "            prediction = model.predict(train_matrix[i], j, i)\n",
    "            # print(prediction, i, j)\n",
    "            rmse_loss += (prediction - expert_matrix[i, j])**2\n",
    "            count += 1\n",
    "    rmse_loss = np.sqrt(rmse_loss/count)\n",
    "    \n",
    "    return rmse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2, 3, 5]:\n",
    "  user_cf = UserBasedCollaborativeFilter(train_matrix, i, 'regular_average')\n",
    "  answers_dict[(i, 'user_regular_average')] = test_user_collaborator(train_matrix, expert_matrix, test_users_start, test_tags_start, user_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2, 3, 5]:\n",
    "  user_cf = UserBasedCollaborativeFilter(train_matrix, i, 'weighted')\n",
    "  answers_dict[(i, 'user_weighted')] = test_user_collaborator(train_matrix, expert_matrix, test_users_start, test_tags_start, user_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ItemBasedCollaborativeFilter` class implements the collaborative filtering algorithm based on items. It will inherit from `CollaborativeFilter` and implement the methods.It computes the similarity between items based on the ratings given by the users. It then predicts the rating of a user for an item by taking `k` most similar items and computing the appropriate average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemBasedCollaborativeFilter(CollaborativeFilter):\n",
    "    def __init__(self,expert_matrix ,k, function):\n",
    "      \"\"\"\n",
    "      utility_matrix : 2D numpy array\n",
    "      k : int\n",
    "      \n",
    "      k is the number of similar items to consider for prediction\n",
    "      utility_matrix is the matrix of user ratings, nan filled with 0\n",
    "      \"\"\"\n",
    "      super().__init__(expert_matrix, function)\n",
    "      self.k = k\n",
    "      self.sim = dict()\n",
    "    \n",
    "    def compute_similarities(self, item_vector : np.ndarray, index: int):\n",
    "      \"\"\"\n",
    "      item_vector : 1D numpy array\n",
    "\n",
    "      \"\"\"\n",
    "      if index in self.sim:\n",
    "        return self.sim[index]\n",
    "      utility_rating_means = np.nanmean(self.utility_matrix.T, axis=1)\n",
    "      item_rating_mean = np.nanmean(item_vector)\n",
    "      \n",
    "      utility_matrix_centered = self.utility_matrix.T - utility_rating_means[:, np.newaxis]\n",
    "\n",
    "      item_vector_centered = item_vector - item_rating_mean\n",
    "\n",
    "      item_vector_norm = np.sqrt(np.nansum(item_vector_centered**2))\n",
    "      utility_matrix_norm = np.sqrt(np.nansum(utility_matrix_centered**2, axis=1))\n",
    "\n",
    "      dot_product = np.nansum(utility_matrix_centered *item_vector_centered, axis=1)\n",
    "\n",
    "      similarities = dot_product/(item_vector_norm * utility_matrix_norm + self.epsilon)\n",
    "\n",
    "      self.sim[index] = similarities\n",
    "\n",
    "      return similarities\n",
    "    \n",
    "    \n",
    "    def find_k_nearest_items(self, item_vector : np.ndarray, u: int, index: int):\n",
    "      \"\"\"\n",
    "      item_vector : 1D numpy array\n",
    "      \n",
    "      returns the indices of the k most similar items to item_vector\n",
    "      \"\"\"\n",
    "      \n",
    "      similarities = self.compute_similarities(item_vector, index)\n",
    "      items_to_consider = np.where(~np.isnan(self.utility_matrix[u]))[0]\n",
    "      k = min(self.k, len(items_to_consider))\n",
    "      sorted_indices = np.argsort(similarities[items_to_consider])[-k:]\n",
    "      items_to_consider = items_to_consider[sorted_indices]\n",
    "      similarities = similarities[items_to_consider]\n",
    "      return similarities, items_to_consider\n",
    "    \n",
    "    \n",
    "    def predict(self, item_vector : np.ndarray, u: int, index: int):\n",
    "      \"\"\"\n",
    "      item_vector : 1D numpy array\n",
    "      u : int\n",
    "      u is the index of the user to predict the rating for\n",
    "      \"\"\"\n",
    "      similarity_scores, items_to_consider = self.find_k_nearest_items(item_vector, u, index)\n",
    "      \n",
    "      # predict the rating for the item\n",
    "      prediction = self.final_rating_function(self.utility_matrix[u, items_to_consider], similarity_scores)\n",
    "      return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tests on the model\n",
    "def test_item_item_filter(train_matrix: np.ndarray, expert_matrix: np.ndarray, user_start: int, item_start: int, model: ItemBasedCollaborativeFilter) -> float:\n",
    "    rmse_loss = 0\n",
    "    count = 0\n",
    "    for i in range(user_start, expert_matrix.shape[0]):\n",
    "        for j in range(item_start, expert_matrix.shape[1]):\n",
    "            prediction = model.predict(train_matrix[:, j], i, j)\n",
    "            # print(prediction, i, j)\n",
    "            rmse_loss += (prediction - expert_matrix[i, j])**2\n",
    "            count += 1\n",
    "    rmse_loss = np.sqrt(rmse_loss/count)\n",
    "    \n",
    "    return rmse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2, 3, 5]:\n",
    "  item_cf = ItemBasedCollaborativeFilter(train_matrix, i, 'regular_average')\n",
    "  answers_dict[(i, 'item_regular_average')] = test_item_item_filter(train_matrix, expert_matrix, test_users_start, test_tags_start, item_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2, 3, 5]:\n",
    "  item_cf = ItemBasedCollaborativeFilter(train_matrix, i, 'weighted')\n",
    "  answers_dict[(i, 'item_weighted')] = test_item_item_filter(train_matrix, expert_matrix, test_users_start, test_tags_start, item_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LatentFactorDecomposition` uses matrix factorization/SVD to predict the ratings of users for items. It decomposes the user-item matrix into two matrices, one for users and one for items. It then predicts the rating of a user for an item by taking the dot product of the corresponding user and item vectors. Each user and item is represented by a vector of latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorDecomposition:\n",
    "    def __init__(self, utility_matrix: np.ndarray, f: int, regp: int= 0, regq: int = 0):\n",
    "        \"\"\"\n",
    "        utility_matrix: 2D numpy array, Expert matrix\n",
    "        f: Number of Latent Factors\n",
    "        regp: regularization rate over p\n",
    "        regq: regularization rate over q\n",
    "        \"\"\"\n",
    "        self.utility_matrix = utility_matrix\n",
    "        self.f = f\n",
    "        self.epsilon = 1e-9\n",
    "        self.regp = regp\n",
    "        self.regq = regq\n",
    "        num_users, num_items = utility_matrix.shape\n",
    "        self.P = np.random.randn(num_users, f)/np.sqrt(num_users*f)\n",
    "        self.Q = np.random.randn(num_items, f)/np.sqrt(num_items*f)\n",
    "\n",
    "    def predict(self, u, i):\n",
    "        return np.dot(self.P[u], self.Q[i])\n",
    "\n",
    "    def train(self, epochs = 1000, alpha = 0.0005):\n",
    "        for epoch in range(epochs):\n",
    "            pred_matrix = np.dot(self.P, self.Q.T)\n",
    "            diffs = pred_matrix - self.utility_matrix\n",
    "            loss = np.nansum(diffs**2) + self.regp*np.nansum(self.P**2) + self.regq*np.nansum(self.Q**2)\n",
    "\n",
    "            p_reg_changes = 2*self.regp*self.P \n",
    "            q_reg_changes = 2*self.regq*self.Q\n",
    "\n",
    "\n",
    "            diffs[np.isnan(diffs)] = 0\n",
    "            # from observation each p[i,j] affects prediction of only user i for each item k. \n",
    "            # So, its GD will get changes for user i and each item k.\n",
    "            p_rmse_changes = 2 * (np.dot(diffs, self.Q))\n",
    "            # from observation each q[i,j] affects prediction of only item i for each user k. \n",
    "            # So, its GD will get changes for item i and each user k.\n",
    "            q_rmse_changes = 2*(np.dot(diffs.T, self.P))\n",
    "\n",
    "            self.P -= alpha*(p_rmse_changes + p_reg_changes)\n",
    "            self.Q -= alpha*(q_rmse_changes + q_reg_changes)\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} : Loss = {loss}\")\n",
    "            \n",
    "    def get_factors(self):\n",
    "        return self.P, self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tests on the model\n",
    "def test_lfd(model: LatentFactorDecomposition, expert_matrix: np.ndarray, user_start: int, item_start : int,f = 5):\n",
    "    rmse_loss = 0\n",
    "    count = 0\n",
    "    for i in range(user_start, expert_matrix.shape[0]):\n",
    "        for j in range(item_start, expert_matrix.shape[1]):\n",
    "            prediction = model.predict(i, j)\n",
    "            # print(prediction, i, j)\n",
    "            rmse_loss += (prediction - expert_matrix[i, j])**2\n",
    "            count += 1\n",
    "    rmse_loss = np.sqrt(rmse_loss/count)\n",
    "    return rmse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 : Loss = 108682.45423809403\n",
      "Epoch 51/200 : Loss = 54548.79407066548\n",
      "Epoch 101/200 : Loss = 50925.957594271145\n",
      "Epoch 151/200 : Loss = 49350.73444101097\n",
      "0.2285103252763705\n",
      "Epoch 1/200 : Loss = 108682.46646106955\n",
      "Epoch 51/200 : Loss = 54121.234991069454\n",
      "Epoch 101/200 : Loss = 41586.83060232552\n",
      "Epoch 151/200 : Loss = 36155.67962402595\n",
      "0.21408036037841702\n",
      "Epoch 1/200 : Loss = 108682.29243428483\n",
      "Epoch 51/200 : Loss = 54330.75544438642\n",
      "Epoch 101/200 : Loss = 38857.484483217304\n",
      "Epoch 151/200 : Loss = 32282.319704725876\n",
      "0.21306155966748108\n"
     ]
    }
   ],
   "source": [
    "for i in [2, 5, 10]:\n",
    "  model = LatentFactorDecomposition(train_matrix, f = i)\n",
    "  model.train(epochs=200)\n",
    "  answers_dict[(i, 'SVD_Without')] = test_lfd(model, expert_matrix, test_users_start, test_tags_start)\n",
    "  print(answers_dict[(i, 'SVD_Without')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 : Loss = 108683.39604270893\n",
      "Epoch 51/200 : Loss = 54448.67166662742\n",
      "Epoch 101/200 : Loss = 47305.95672149307\n",
      "Epoch 151/200 : Loss = 47124.285467495196\n",
      "0.22091363150362173\n",
      "Epoch 1/200 : Loss = 108682.33936755583\n",
      "Epoch 51/200 : Loss = 54333.64864867695\n",
      "Epoch 101/200 : Loss = 40333.4339028831\n",
      "Epoch 151/200 : Loss = 36138.45988096543\n",
      "0.2131792184625132\n",
      "Epoch 1/200 : Loss = 108681.94093299267\n",
      "Epoch 51/200 : Loss = 54240.34315062918\n",
      "Epoch 101/200 : Loss = 38492.98363717051\n",
      "Epoch 151/200 : Loss = 32780.67853778636\n",
      "0.21557558741291857\n"
     ]
    }
   ],
   "source": [
    "for i in [2, 5, 10]:\n",
    "  model = LatentFactorDecomposition(train_matrix, f = i, regp=0.001, regq=0.003)\n",
    "  model.train(epochs=200)\n",
    "  answers_dict[(i, 'SVD_With_0.001_0.003')] = test_lfd(model, expert_matrix, test_users_start, test_tags_start)\n",
    "  print(answers_dict[(i, 'SVD_With_0.001_0.003')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 : Loss = 108683.27350784901\n",
      "Epoch 51/200 : Loss = 54241.24522602493\n",
      "Epoch 101/200 : Loss = 47142.114327056915\n",
      "Epoch 151/200 : Loss = 47128.09891241025\n",
      "0.22145298235078978\n",
      "Epoch 1/200 : Loss = 108681.74177835233\n",
      "Epoch 51/200 : Loss = 54181.20487920439\n",
      "Epoch 101/200 : Loss = 41491.689355902556\n",
      "Epoch 151/200 : Loss = 36177.78008982759\n",
      "0.21433817310380507\n",
      "Epoch 1/200 : Loss = 108682.04215148147\n",
      "Epoch 51/200 : Loss = 54033.50538528124\n",
      "Epoch 101/200 : Loss = 38334.93551883678\n",
      "Epoch 151/200 : Loss = 32244.997886542253\n",
      "0.21469486749449163\n"
     ]
    }
   ],
   "source": [
    "for i in [2, 5, 10]:\n",
    "  model = LatentFactorDecomposition(train_matrix, f = i, regp=0.05, regq=0.05)\n",
    "  model.train(epochs=200)\n",
    "  answers_dict[(i, 'SVD_With_0.05_0.05')] = test_lfd(model, expert_matrix, test_users_start, test_tags_start)\n",
    "  print(answers_dict[(i, 'SVD_With_0.05_0.05')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 : Loss = 108684.39489216299\n",
      "Epoch 51/200 : Loss = 54798.230606556484\n",
      "Epoch 101/200 : Loss = 48035.61950506613\n",
      "Epoch 151/200 : Loss = 47562.070135485345\n",
      "0.22206790184243405\n",
      "Epoch 1/200 : Loss = 108683.57499126432\n",
      "Epoch 51/200 : Loss = 54653.683362976444\n",
      "Epoch 101/200 : Loss = 40209.25805401928\n",
      "Epoch 151/200 : Loss = 37575.42367985966\n",
      "0.21403378723754646\n",
      "Epoch 1/200 : Loss = 108682.97417545425\n",
      "Epoch 51/200 : Loss = 54290.531064528564\n",
      "Epoch 101/200 : Loss = 39107.82921752086\n",
      "Epoch 151/200 : Loss = 33591.80624163666\n",
      "0.220214371145173\n"
     ]
    }
   ],
   "source": [
    "for i in [2, 5, 10]:\n",
    "  model = LatentFactorDecomposition(train_matrix, f = i, regp=0.5, regq=0.75)\n",
    "  model.train(epochs=200)\n",
    "  answers_dict[(i, 'SVD_With_0.5_0.75')] = test_lfd(model, expert_matrix, test_users_start, test_tags_start)\n",
    "  print(answers_dict[(i, 'SVD_With_0.5_0.75')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 'user_regular_average'): 0.2523897746165076,\n",
       " (3, 'user_regular_average'): 0.24193819276795364,\n",
       " (5, 'user_regular_average'): 0.2323844213545756,\n",
       " (2, 'user_weighted'): 0.2522930769032603,\n",
       " (3, 'user_weighted'): 0.2428522206789541,\n",
       " (5, 'user_weighted'): 0.23381228893778747,\n",
       " (2, 'item_regular_average'): 0.2519409772599003,\n",
       " (3, 'item_regular_average'): 0.25313168238934886,\n",
       " (5, 'item_regular_average'): 0.2597371586659773,\n",
       " (2, 'item_weighted'): 0.25207872457326164,\n",
       " (3, 'item_weighted'): 0.25250284085752034,\n",
       " (5, 'item_weighted'): 0.25014382125273715,\n",
       " (2, 'SVD_Without'): 0.2285103252763705,\n",
       " (5, 'SVD_Without'): 0.21408036037841702,\n",
       " (10, 'SVD_Without'): 0.21306155966748108,\n",
       " (2, 'SVD_With_0.001_0.003'): 0.22091363150362173,\n",
       " (5, 'SVD_With_0.001_0.003'): 0.2131792184625132,\n",
       " (10, 'SVD_With_0.001_0.003'): 0.21557558741291857,\n",
       " (2, 'SVD_With_0.05_0.05'): 0.22145298235078978,\n",
       " (5, 'SVD_With_0.05_0.05'): 0.21433817310380507,\n",
       " (10, 'SVD_With_0.05_0.05'): 0.21469486749449163,\n",
       " (2, 'SVD_With_0.5_0.75'): 0.22206790184243405,\n",
       " (5, 'SVD_With_0.5_0.75'): 0.21403378723754646,\n",
       " (10, 'SVD_With_0.5_0.75'): 0.220214371145173}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/dikshant/.local/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-surprise in /home/dikshant/.local/lib/python3.10/site-packages (1.1.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/dikshant/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/dikshant/.local/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/dikshant/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dikshant/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "from surprise import Dataset, SVD\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(train_matrix)\n",
    "long_df = dataset_df.stack().reset_index()\n",
    "long_df.columns = ['user_id', 'item_id', 'rating']\n",
    "# Define reader with rating scale (adjust if necessary)\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "# Load into Surprise dataset\n",
    "train_data = Dataset.load_from_df(long_df, reader)\n",
    "# Build the full training set\n",
    "trainset = train_data.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(986.0, 827.0, 0.0),\n",
       " (986.0, 828.0, 0.0),\n",
       " (986.0, 829.0, 0.0),\n",
       " (986.0, 830.0, 0.0),\n",
       " (986.0, 831.0, 0.0),\n",
       " (986.0, 832.0, 0.0),\n",
       " (986.0, 833.0, 0.0),\n",
       " (986.0, 834.0, 0.0),\n",
       " (986.0, 835.0, 0.0),\n",
       " (986.0, 836.0, 0.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating test data in format of surprise dataset\n",
    "test_matrix = np.copy(expert_matrix)\n",
    "test_df = pd.DataFrame(test_matrix)\n",
    "test_long_df = test_df.stack().reset_index()\n",
    "test_long_df.columns = ['user_id', 'item_id', 'rating']\n",
    "test_df_final = test_long_df[(test_long_df['user_id'] >= test_users_start) & (test_long_df['item_id'] >= test_tags_start)].reset_index(drop=True)\n",
    "test_data = Dataset.load_from_df(test_df_final, reader)\n",
    "testset = [tuple(x) for x in test_df_final[['user_id', 'item_id', 'rating']].values]\n",
    "testset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_surprise_colaborative_filter(user_based=True,k =2):\n",
    "    model = KNNBasic(sim_options = {'name': 'pearson', 'user_based': user_based}, k=k)\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    return accuracy.rmse(predictions)\n",
    "\n",
    "def test_surprise_svd():\n",
    "    model = SVD()\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    return accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_dict= dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This takes a lot of time to run, so commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing user based collaborative filter\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.2511\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.2409\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.2266\n",
      "Testing item based collaborative filter\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.2521\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.2525\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.2501\n",
      "Testing SVD\n",
      "RMSE: 0.2247\n",
      "RMSE: 0.2248\n",
      "RMSE: 0.2247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # testing user based collaborative filter\n",
    "# print(\"Testing user based collaborative filter\")\n",
    "# for i in [2, 3, 5]:\n",
    "#     surprise_dict[(i, 'user_based')] = test_surprise_colaborative_filter(user_based=True, k=i)\n",
    "    \n",
    "# # testing item based collaborative filter\n",
    "# print(\"Testing item based collaborative filter\")\n",
    "# for i in [2, 3, 5]:\n",
    "#     surprise_dict[(i, 'item_based')] = test_surprise_colaborative_filter(user_based=False, k=i)\n",
    "    \n",
    "# # testing SVD\n",
    "# print(\"Testing SVD\")\n",
    "# for i in [2, 5, 10]:\n",
    "#     surprise_dict[(i, 'SVD')] = test_surprise_svd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'user_based') 0.2511263447799646\n",
      "(3, 'user_based') 0.2408922378736791\n",
      "(5, 'user_based') 0.22657016945231917\n",
      "(2, 'item_based') 0.25207872457284747\n",
      "(3, 'item_based') 0.25250284085757\n",
      "(5, 'item_based') 0.25014613646770295\n",
      "(2, 'SVD') 0.22467881155312133\n",
      "(5, 'SVD') 0.22479555494685505\n",
      "(10, 'SVD') 0.22472219224050763\n"
     ]
    }
   ],
   "source": [
    "for key, val in surprise_dict.items():\n",
    "  print(key, val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
